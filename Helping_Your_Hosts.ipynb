{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏡 **Helping Your Hosts:** Predicting Airbnb Host Ratings 🏨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Phase 3 Project: Classification**\n",
    ">\n",
    "> **Author:** Ben McCarty\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**In a post-COVID world, hospitality faces challenges as travel restrictions are imposed and lifted (and then re-imposed).** Travel and tourism came to a crashing halt in 2020 and still face challenges in returning to pre-2020 business levels.\n",
    "\n",
    "As restless travelers look to escape the confines of their homes, they expect the same high-quality services and experiences as pre-COVID. Competition within the hospitality industry is stronger than ever, putting more pressure on businesses to keep and grow their customer base.\n",
    "\n",
    "**The main performance metric for every company involved in hospitality is guest satisfaction.** If a guest isn't satisfied, they are not likely to return for another visit and may share their experience with others, pushing away potential business.\n",
    "\n",
    "Airbnb hosts face the same challenges as traditional hotels in these aggressive and challenging market conditions. In order to maximize their profitability and to distinguish themselves from traditional hotels, **Airbnb needs to know which aspects of a host property are the strongest predictors of whether a guest will give a satisfaction score of 4.8 or higher (out of 5).**\n",
    "\n",
    "With this question in mind, I obtained data about Airbnb host properties from the [Inside Airbnb project](http://insideairbnb.com/get-the-data.html#:~:text=Washington%2C%20D.C.%2C%20District%20of%20Columbia%2C%20United%20States) for the Washington, D.C. area. The dataset includes details about the hosts themselves; property details (bedrooms, bathrooms, property types); and reservation availability.\n",
    "\n",
    "**Once I have the data readied, I will use machine learning modeling techniques to determine my most important features for the region.** Then I will provide my final recommendations on what Airbnb should do to maximize the likelihood of their hosts obtaining a score of 4.8 or greater.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📂 **Imports and Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools to reload functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:37.144462Z",
     "start_time": "2021-10-14T01:00:35.269385Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact_manual\n",
    "import missingno\n",
    "import shap\n",
    "\n",
    "## Personal functions\n",
    "from bmc_functions import eda\n",
    "from bmc_functions import classification as clf\n",
    "\n",
    "## Settings\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speeding up SKLearn via Intel(R) Extension for Scikit-learn*\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scikit-Learn\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Show Visualizations Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:37.777580Z",
     "start_time": "2021-10-14T01:00:37.766578Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting to control whether or not to show visualizations\n",
    "show_visualizations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T23:22:16.380574Z",
     "start_time": "2021-08-11T23:22:16.362567Z"
    }
   },
   "source": [
    "# 📖 **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.287606Z",
     "start_time": "2021-10-14T01:00:37.780579Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading data and saving to a DataFrame\n",
    "\n",
    "source = 'data/listings.csv.gz'\n",
    "data = pd.read_csv(source)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.303577Z",
     "start_time": "2021-10-14T01:00:38.290577Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T15:42:29.860581Z",
     "start_time": "2021-08-11T15:42:29.846550Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> The initial read of the dataset shows there are 74 features and 8,033 entries. A quick glance at the `.head()` gives a sample of the entries, showing that some of the features are not relevant to my analysis.\n",
    ">\n",
    "> I need to get a better idea of the statistics for the dataset, especially any missing values and the datatypes for each column. I need to pre-process this data before I can perform any modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👨‍💻 **Interactive Investigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To increase accessibility to the data, **I include a widget to allow the user to sort through the data interactively.** I use [**Jupyter Widgets**](https://ipywidgets.readthedocs.io/en/latest/index.html) to create this interactive report.\n",
    ">\n",
    ">**To use:** select which column by which you would like to sort from the dropdown menu, then click the \"Run Interact\" button.\n",
    ">\n",
    ">***Note about 'Drop_Cols' and Cols:*** these keyword arguments are used to allow the user to drop specific columns.\n",
    ">\n",
    "> **Only click the \"Drop_Cols\" option when specifying \"Cols\"!** Otherwise it will cause an error.\n",
    ">\n",
    ">The 'Cols' dropdown menu does not affect the resulting report; the data is filtered from the report prior to displaying the results. \n",
    ">\n",
    ">I chose to include this option for flexibility and adaptability, but it does have the unintended consequence of creating another drop-down menu. Please ignore this menu, as it does not provide any additional functionality. For future work, I will disable the menu to prevent confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.601576Z",
     "start_time": "2021-10-14T01:00:38.306577Z"
    }
   },
   "outputs": [],
   "source": [
    "## Running report on unfiltered dataset\n",
    "\n",
    "interact_manual(eda.sort_report, Sort_by=list(eda.report_df(data).columns),\n",
    "                Source=source);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.663575Z",
     "start_time": "2021-10-14T01:00:38.603579Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After reviewing my data, I see there are several features that contain irrelevant entries (URLs, source data, meta data) or values that are too complicated for simple processing (such as host and listing descriptions).\n",
    ">\n",
    "> I will drop these columns for the second report to review the remaining data for further processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.679576Z",
     "start_time": "2021-10-14T01:00:38.666577Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying columns to drop\n",
    "\n",
    "drop = ['id', 'host_id', 'name', 'description', 'neighborhood_overview', 'host_name',\n",
    "        'host_about', 'host_location', 'neighbourhood', 'property_type',\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'picture_url','host_url',\n",
    "        'host_thumbnail_url','host_picture_url','calendar_last_scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.695576Z",
     "start_time": "2021-10-14T01:00:38.682578Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data.drop(columns=drop).copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.916576Z",
     "start_time": "2021-10-14T01:00:38.697580Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Creating updated interactive report\n",
    "\n",
    "interact_manual(eda.sort_report, Drop_Cols = True, Cols = drop,\n",
    "                Sort_by=list(eda.report_df(df).columns), Source=source);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "> The report shows that the dataset has a big problem with missing values:\n",
    ">\n",
    ">* **100% Missing:**\n",
    ">   * `neighbourhood_group_cleansed`\n",
    ">   * `bathrooms`\n",
    ">   * `calendar_updated`\n",
    "></br></br>\n",
    "> * **~100% Missing:**\n",
    ">   * `license`\n",
    "></br></br>\n",
    "> * **26-39% Missing:**\n",
    ">   * `host_about`\n",
    ">   * `neighborhood_overview`\n",
    ">   * `neighbourhood`\n",
    ">   * `host_response_time`\n",
    ">   * `host_response_rate`\n",
    ">   * `review_scores_value`\n",
    ">   * `review_scores_checkin`\n",
    ">   * `review_scores_location`\n",
    ">   * `review_scores_accuracy`\n",
    ">   * `review_scores_communication`\n",
    ">   * `review_scores_cleanliness`\n",
    ">   * `host_acceptance_rate`\n",
    ">   * `reviews_per_month`\n",
    ">   * `first_review`\n",
    ">   * `review_scores_rating`\n",
    ">   * `last_review`\n",
    "\n",
    "---\n",
    "\n",
    "**Handling the Missing Values**\n",
    "\n",
    "> I will need to address these missing values before processing with the modeling. My options include:\n",
    ">\n",
    ">\n",
    ">* **Filling missing values with an imputer as part of modeling pipeline**\n",
    ">   * *Allows for the flexibility to test different imputation methods*\n",
    ">   * *Can add a feature to indicate which features were missing values.*</br></br>\n",
    ">\n",
    ">* **Dropping the rows with missing values.**\n",
    ">   * *Reduces the number of features used in modeling, reducing dimensionality*\n",
    ">   * *May decrease model performance due to less information*</br>\n",
    "\n",
    "**I will use a mix of these two options:** I will drop the features missing nearly all of the values, then I will use an imputer during my pipeline process in combination with a GridSearch to identify the best method to use to fill the missing values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**MissingNo**\n",
    "\n",
    "> To get a better idea of the missing values, I create a visual of the values via the 'Missingno' package. This visualization package includes several options for visualizing the missing data.\n",
    "</br></br>\n",
    "> *Note: please set the \"show_visualizations\" variable to \"True\" to show these visualizations. By default they are disabled due to time required for processing.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.931577Z",
     "start_time": "2021-10-14T01:00:38.919577Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visually inspecting missing values\n",
    "if show_visualizations == True:\n",
    "    missingno.bar(data, labels=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:00:38.947587Z",
     "start_time": "2021-10-14T01:00:38.933578Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visually inspecting missing values\n",
    "if show_visualizations == True:\n",
    "    missingno.matrix(data, labels=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on this visualization, I see that **there is a consistent trend in missing values for review scores:** if a row is missing one review score, it seems to be missing all of them.\n",
    ">\n",
    "> Additionally, **there are many missing values for the response time, response rate, and acceptance rate.** I want to use these columns in my classification, so I will need to replace those missing values.\n",
    ">\n",
    "> After reviewing these details, **I feel more comfortable with the option of dropping those rows with missing review values.** I will drop the values as part of my overall classification process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧼 **Data Cleaning and EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Cleaning Process**\n",
    "\n",
    "> To begin the cleaning process, I will:\n",
    ">   * Drop the rows missing values for the target feature\n",
    ">   * Drop features missing 98-100% of values\n",
    "\n",
    "**Processing Remaining Missing Values**\n",
    "\n",
    "> Instead of performing any manual updates to the remaining values, I will test different imputation methods as part of my modeling pipeline. </br></br>\n",
    "> Potential methods would include:\n",
    ">   * Imputing the string \"MISSING\"\n",
    ">   * Imputing the most frequent value for string values\n",
    ">   * Using the mean, median, or mode for numeric datatypes\n",
    ">\n",
    "> The benefit of including this step in a pipeline is that I will be able to include these different methods in a GridSearchCV as part of my hyperparameter turning steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Rows Without Target Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To start my data cleaning and exploration, I will drop any rows missing values for my target feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating number of missing values for target feature\n",
    "display(df['review_scores_rating'].isna().sum())\n",
    "f\"The target feature is missing {round(df['review_scores_rating'].isna().sum() / len(df)*100, 2)}% of its data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:02:16.829913Z",
     "start_time": "2021-10-14T01:02:16.810913Z"
    }
   },
   "outputs": [],
   "source": [
    "## Identifying row indices\n",
    "nan_index = df['review_scores_rating'].isna()\n",
    "nan_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:02:40.552242Z",
     "start_time": "2021-10-14T01:02:40.536205Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping rows by index and resetting index\n",
    "df = df.drop(df[nan_index].index)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:02:40.872071Z",
     "start_time": "2021-10-14T01:02:40.701076Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reviewing results\n",
    "f\"There are {df['review_scores_rating'].isna().sum()} missing values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I dropped the missing values for my target feature, I will review the distribution of my ratings.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:05:33.110511Z",
     "start_time": "2021-10-14T01:05:33.097513Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing the percentage of ratings at or above the threshold of 4.8\n",
    "\n",
    "f\"{round(len(df['review_scores_rating'][df['review_scores_rating'] >= 4.8])/ len(df)*100, 0)} of ratings are at/above the threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:05:35.763596Z",
     "start_time": "2021-10-14T01:05:35.117600Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualizing the overall distribution of ratings\n",
    "\n",
    "ax = sns.histplot(data = df['review_scores_rating'], bins = 'auto')\n",
    "\n",
    "ax.set(title = 'Distribution of All Review Ratings',\n",
    "       xlabel = 'Rating Score', ylabel = 'Number of Ratings')\n",
    "\n",
    "median = df['review_scores_rating'].median()\n",
    "mean = round(df['review_scores_rating'].mean(), 2)\n",
    "ax.axvline(median, label = f'Median Score: {median}', color='g')\n",
    "ax.axvline(mean, label = f'Average Score: {mean}', color='y')\n",
    "ax.axvline(4.8, label = 'Score Threshold: 4.80', color='k')\n",
    "ax.legend(fontsize= 'large',title = 'Score Thresholds',\n",
    "          title_fontsize = 'large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:52:30.415686Z",
     "start_time": "2021-10-14T00:51:53.787206Z"
    }
   },
   "outputs": [],
   "source": [
    "## Zooming in on 4.5 - 5.0 range\n",
    "ax = sns.histplot(data = df['review_scores_rating'][df['review_scores_rating']>4.5], bins = 'auto')\n",
    "ax.set(title = 'Distribution of Review Ratings: 4.5+', xlabel = 'Rating Score', ylabel = 'Number of Ratings')\n",
    "median = df['review_scores_rating'].median()\n",
    "mean = round(df['review_scores_rating'].mean(), 2)\n",
    "ax.axvline(median, label = f'Median Score: {median}', color='g')\n",
    "ax.axvline(mean, label = f'Average Score: {mean}', color='y')\n",
    "ax.axvline(4.8, label = 'Score Threshold: 4.80', color='k')\n",
    "ax.legend(fontsize= 'large',title = 'Score Thresholds',title_fontsize = 'large');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Observations and Next Steps**\n",
    "\n",
    "> Based on the results above, I see that **62% of the reviews are at or above the target threshold of 4.8.**\n",
    ">\n",
    "> These scores show that there's a close balance of scores that are meeting our threshold. However, this imbalance may still impair the performance of my future model.\n",
    ">\n",
    "> To address this imbalance, I will later use the SMOTE technique to oversample the minority class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔨 Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Selecting Features to Drop**\n",
    "\n",
    "> * Dropping:\n",
    ">    * all features beginning with \"review\" except target (these review scores constitute the target)\n",
    ">    * features missing nearly all/all values\n",
    ">    * `host_is_superhost` - status requires score of 4.8+\n",
    ">    * `amenities` - string containing listing-specific amenities; removed due to non-standard/varied data adding little extra value to the model\n",
    ">        * **Future work:** using NLP to identify the most common amenities to create more valuable features\n",
    ">    * `host_neighborhood` - `neighbourhood_cleansed` is more substantial and covers the same data\n",
    ">    * `first_review` - does not add significant value\n",
    "</br></br>\n",
    "\n",
    "**Engingeering New Features**\n",
    "\n",
    ">    * `reviewed_within_year`: boolean column representing whether or not the reservation was booked within a year from the specified date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating list of columns to drop starting with features beginning with \"review_\" (except the target feature)\n",
    "drop_feats = df.loc[:, df.columns.str.startswith('review_')].drop(columns='review_scores_rating').columns.to_list()\n",
    "\n",
    "## Adding additional features as described above\n",
    "drop_feats.extend(['host_is_superhost',\"amenities\", 'host_neighbourhood', 'first_review'])\n",
    "drop_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding column names for features missing more than 90% of their data\n",
    "drop_feats.extend(df.isna().mean()[df.isna().mean() > .9].index.to_list())\n",
    "drop_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping selected features\n",
    "df = df.drop(columns=drop_feats)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `reviewed_within_year` Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determining the date a year ago\n",
    "selected_date = datetime.datetime(2021, 10, 14)\n",
    "last_year = (selected_date-datetime.timedelta(days=365))\n",
    "last_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating boolean feature representing if the last review was within a year of the specified date\n",
    "df['reviewed_within_year'] = (pd.to_datetime(df['last_review']) >= last_year).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping 'last_review' due to multicollinearity concerns\n",
    "df = df.drop(columns='last_review')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `years_hosting`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Using the `host_since` feature, I will create a new feature representing each host's years of experience.\n",
    ">\n",
    "> After creating the new feature, I will drop the `host_since` feature.\n",
    ">\n",
    "> ***Special Note:*** Due to the presence of a limited number of missing values, I will temporarily fill the missing values with a placeholder to create the features, then re-convert the placeholders to NaN values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating initial number of missing values\n",
    "df[\"host_since\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying indices for missing values\n",
    "host_since_nan = df[\"host_since\"][df[\"host_since\"].isna()].index\n",
    "host_since_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determining unique placeholder\n",
    "f'Is the value \"2000-01-01\" in the column? {len(df[\"host_since\"][df[\"host_since\"].isin([\"2000-01-01\"])])>0}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filling with placeholder value\n",
    "df[\"host_since\"] = df[\"host_since\"].fillna('2000-01-01')\n",
    "f'There are {df[\"host_since\"].isna().sum()} missing values.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.114481Z",
     "start_time": "2021-10-14T00:20:55.031Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating feature\n",
    "df['years_hosting'] = df[\"host_since\"].map(lambda x: 2021- int(x.split(\"-\")[0]))\n",
    "df['years_hosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirm post-conversion results for origina missing rows\n",
    "df.loc[host_since_nan, [\"host_since\", 'years_hosting']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting back to missing values\n",
    "df.loc[host_since_nan, [\"host_since\", 'years_hosting']] = np.nan\n",
    "df.loc[host_since_nan, [\"host_since\", 'years_hosting']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.116480Z",
     "start_time": "2021-10-14T00:20:55.035Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Confirming final values\n",
    "print(df['years_hosting'].value_counts(dropna=False))\n",
    "eda.report_df(df).loc['years_hosting':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping `host_since` feature after update\n",
    "df = df.drop(columns='host_since')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: Creating `Host_Since` Feature**\n",
    "\n",
    "> I successfully created the new feature representing each host's years of experience (up to 2021).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 **Fixing Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After dropping certain features and creating new ones, I will process the remaining features and columns to allow for the modeling process.\n",
    ">\n",
    "> I perform the following changes:</br></br>\n",
    "\n",
    "> **Data Conversions:**\n",
    ">   * **Binarizing target:** currently, my target feature `review_scores_rating` is a range of values \n",
    ">   * **T/F values:** Any features with 't'/'f' values need to be converted to 1/0, respectively.\n",
    ">   * **`price`:** The `price` feature consists of string values; to use it properly, I will convert the values to the float datatype.\n",
    ">   * **`room_type`:** Converting to simpler string values.\n",
    ">   * **`neighbourhood_cleansed`:** The 'neighbourhood_cleansed' feature values are a single string of neighborhoods. I will split these strings into boolean features for each neighborhood. </br></br>\n",
    "\n",
    "> **Feature Engineering:**\n",
    ">   * `years_hosting`: Using the year in which the host started in the `host_since` feature to calculate the number of years as a host.\n",
    ">   * `bathrooms_text`: Converting to a new `num_bathrooms` numeric feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Binarizing Target Feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In order to achieve the goal of identifying the most important features for review scores, I convert the target variable 'review_scores_rating\" into binary values to represent if the score is below the threshold of 4.8 (represented as a '0') and above the threshold (represented as a '0').\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.270484Z",
     "start_time": "2021-10-14T00:20:55.232Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using np.select to reassign target values based on conditional evaluations\n",
    "\n",
    "cond = [df['review_scores_rating'] >= 4.8,\n",
    "        df['review_scores_rating'] < 4.8]\n",
    "\n",
    "choice = [1,0]\n",
    "\n",
    "df['meets_score_threshold'] = np.select(cond, choice, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.272479Z",
     "start_time": "2021-10-14T00:20:55.237Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing results to confirm only 0/1 values and inspecting balance\n",
    "df['meets_score_threshold'].value_counts(dropna=False, normalize=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping old feature\n",
    "df = df.drop(columns= 'review_scores_rating')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After processing the missing values and formatting the data, the values are properly converted into 0/1 values and the class balance is maintained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting True/False Columns to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting columns containing string values\n",
    "df.select_dtypes('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identifying columns with 't' values - also includes 'f' values\n",
    "t_f_col = df.loc[:,(df == 't').any()].columns\n",
    "t_f_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.106481Z",
     "start_time": "2021-10-14T00:20:55.002Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting t/f to 1/0, respectively\n",
    "df.loc[:,t_f_col] = df.loc[:,t_f_col].replace({ 't' : 1, 'f' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.108479Z",
     "start_time": "2021-10-14T00:20:55.006Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:,t_f_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confirming values\n",
    "display(df['host_has_profile_pic'].value_counts(dropna=False))\n",
    "display(df['host_identity_verified'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.111482Z",
     "start_time": "2021-10-14T00:20:55.016Z"
    }
   },
   "outputs": [],
   "source": [
    "## Verifying results\n",
    "eda.report_df(df[t_f_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: Binarizing True/False**\n",
    "\n",
    "> The `t` and `f` values are now properly converted to binary values. The pipeline imputer will resolve the remaining missing values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Price to Float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting original feature\n",
    "df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.112494Z",
     "start_time": "2021-10-14T00:20:55.021Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each string value into a float\n",
    "df['price'] = df['price'].map(lambda price: price[1:].replace(',','')).astype('float')\n",
    "df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.113482Z",
     "start_time": "2021-10-14T00:20:55.025Z"
    }
   },
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: Prices to Floats**\n",
    "\n",
    "> The `price` feature is now a float instead of string value.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting `Host_Response_Time` and `Host_Acceptance_Rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Response and Acceptance Rates**\n",
    "\n",
    "> Currently the `Host_Response_Time` and `Host_Acceptance_Rate` features consistsof string values representing percentages.\n",
    ">\n",
    "> **I will convert these values to float values.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iterating through selected features and filling missing values\n",
    "\n",
    "feat_list = ['host_acceptance_rate', 'host_response_rate']\n",
    "\n",
    "placeholder = '999%'\n",
    "\n",
    "for x in feat_list:\n",
    "\n",
    "    print(f'***'*5, f'\\nFeature: {x}:\\n')\n",
    "    ## Checking for Missing Values\n",
    "    print(f\"There are {df[x].isna().sum()} missing values.\")\n",
    "\n",
    "    ## Determining placeholder value to fill missing values prior to conversion.\n",
    "    print(f\"Is the value '{placeholder}' in '{x}?' {df[x].isin([{placeholder}]).sum() >0}\")\n",
    "\n",
    "    if (df['host_acceptance_rate'].isin([{'999%'}]).sum() >0) == False:\n",
    "        ## Filling missing values with placeholder\n",
    "        df[x] = df[x].fillna(placeholder)\n",
    "        print(f'Filled missing values in {x} with {placeholder}.')\n",
    "    else:\n",
    "        print('Please select a different placeholder value.')\n",
    "\n",
    "    print(f\"There are {df[x].isna().sum()} missing values remaining.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing '%' and converting to floats\n",
    "for feat in feat_list:\n",
    "    df[feat] = df[feat].map(lambda x: float(x.replace('%',''))*.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting placeholder back to NaN\n",
    "placeholder = int(placeholder.replace(\"%\",''))*.01\n",
    "\n",
    "for x in feat_list:\n",
    "        df[x] = df[x].replace(9.99, np.nan)\n",
    "        display(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewing final updates\n",
    "df[feat_list].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conculsion: Converting Percentages**\n",
    "\n",
    "> The '`host_acceptance_rate`', '`host_response_rate`' features are now properly listed as float values, more accurately representing the percentage values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❌ Filling Beds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**`Beds` and `Bedrooms`**\n",
    "\n",
    "> The `beds` and `bedrooms` features are both missing values, and instead of using an imputer or dropping the columns, I will take a different approach: filling the missing values with the values from the other feature. As these two features are very similar (you would expect a bed to be in a bedroom, and while ), I feel it is acceptable to take this approach.\n",
    ">\n",
    "> As the values are similar between the two, I will compare the rows against each other. For each row, if there is a missing value in one column that is present in the other, I will fill the missing value with the value present in the other column.\n",
    "\n",
    "\n",
    "These two columns represent similar data and both are missing values.\n",
    "\n",
    "My first approach to fill the missing values involved using the value from each respective feature to fill the missing values in the other. However, I realized that this approach may not represent cases in which a listing may be for a shared room (like a hostel, for example).\n",
    "\n",
    "**I will inspect the rows in which there is a value for `beds` but not `bedrooms` to determine if there are any listings that are not including a full bedroom.**\n",
    "\n",
    "**For any cases in which this is not the case, I will simply use each feature's values to fill in any missing values for the other.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.report_df(df)[eda.report_df(df)['null_sum'] >0].sort_values('null_sum', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda.report_df(df).sort_values('null_sum', ascending=False).loc[['beds', 'bedrooms'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting room types and number of beds detailed for listings without values for 'bedrooms'\n",
    "display(df['room_type'][df['bedrooms'].isna()].value_counts(dropna=False, normalize=True))\n",
    "display(df['beds'][df['bedrooms'].isna()].value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "These results show that listings that are missing the number of bedrooms are most often entire homes/apartments with either 1 or 2 bathrooms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bedrooms'][df['room_type'] == 'Entire home/apt'].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:03:20.856897Z",
     "start_time": "2021-10-14T01:03:20.840896Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Filling missing values for 'beds' with values for 'bedrooms'\n",
    "# df['beds'].fillna(df['bedrooms'], inplace=True)\n",
    "# df['beds'].isna().sum()\n",
    "\n",
    "# ## Filling missing values for 'bedrooms' with values for 'beds'\n",
    "# df['bedrooms'].fillna(df['beds'], inplace=True)\n",
    "\n",
    "# df['beds'].isna().sum(), df['bedrooms'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T01:03:26.898106Z",
     "start_time": "2021-10-14T01:03:23.145953Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## Confirming reduction in missing values for 'beds' and 'bedrooms'\n",
    "\n",
    "# # rpt_clean  = eda.report_df(df)\n",
    "# # rpt_clean[rpt_clean['null_sum'] >0].sort_values('null_sum', ascending=False)\n",
    "\n",
    "# f\"There are {df['beds'].isna().sum()} missing value(s) for `beds` and {df['bedrooms'].isna().sum()} missing value(s) for `bedrooms`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:54:07.430264Z",
     "start_time": "2021-10-14T00:54:07.420232Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## Inspecting row with missing value for \"bed\"\n",
    "# df['beds'][df['beds'].isna() >0], df['bedrooms'][df['bedrooms'].isna() >0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting `Bathrooms_Text` to `Num_Bathrooms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Remaking `Bathrooms` Feature**\n",
    "\n",
    "> As part of my initial pre-processing steps, I removed features missing 90%+ of their data, including the `bathrooms` feature. However, the \"`bathrooms_text` feature covers similar data as string values. To use this data, I will need to convert it to numeric data.\n",
    "\n",
    "**I will convert the `bathrooms_text` feature to a new feature, `num_bathrooms`, to create a \"new\" feature based on the numeric values from the original text strings.**\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspecting the values for \"bathrooms_text\"\n",
    "df['bathrooms_text'].value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.122487Z",
     "start_time": "2021-10-14T00:20:55.057Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the rows in which there are null values\n",
    "df[df['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.123480Z",
     "start_time": "2021-10-14T00:20:55.062Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling null values with unique string ('999' not present otherwise)\n",
    "df['bathrooms_text'].fillna('999', inplace=True)\n",
    "\n",
    "## Verifying all null values are filled\n",
    "df['bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewing updated values\n",
    "df['bathrooms_text'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.127497Z",
     "start_time": "2021-10-14T00:20:55.074Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting each string value and selecting the first element (representing the number of bathrooms)\n",
    "df['num_bathrooms'] = df['bathrooms_text'].map(lambda x: x.split(' ')[0])\n",
    "df['num_bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting remaining string values to floats\n",
    "df['num_bathrooms'] = df['num_bathrooms'].replace({'Shared': .05, \"Half-bath\": .05})\n",
    "df['num_bathrooms'] = df['num_bathrooms'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_bathrooms'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.240478Z",
     "start_time": "2021-10-14T00:20:55.217494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting rows where 'num_bathrooms' is zero to validate data\n",
    "df[df['num_bathrooms'] ==0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.242480Z",
     "start_time": "2021-10-14T00:20:55.104Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing old column post-conversion\n",
    "df = df.drop(columns = 'bathrooms_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.244480Z",
     "start_time": "2021-10-14T00:20:55.110Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming removal\n",
    "'bathrooms_text' in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_bathrooms'] = df['num_bathrooms'].replace(999.00, np.nan)\n",
    "df['num_bathrooms'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Conclusion: Converting `Bathroom_Text` to `Num_Bathrooms`\n",
    "\n",
    "> I successfully created the new feature, `num_bathrooms`, and retained the four missing values for later imputation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Room_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Cleaning `Room_Type` for Encoding**\n",
    "\n",
    ">  In order to use `room_type` as a categorical variable, I will standardize the string values to be encoded during my pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.244480Z",
     "start_time": "2021-10-14T00:20:55.120Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing pre-existing values\n",
    "df['room_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.246479Z",
     "start_time": "2021-10-14T00:20:55.128Z"
    }
   },
   "outputs": [],
   "source": [
    "## Replacing values with updated strings\n",
    "\n",
    "replace_rooms = {'Entire home/apt': 'entire_home', \n",
    "                 'Private room': 'private_room',\n",
    "                 'Shared room': 'shared_room',\n",
    "                 'Hotel room': 'hotel_room'\n",
    "                }\n",
    "\n",
    "df['room_type'].replace(replace_rooms, inplace=True)\n",
    "df['room_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: `Room_Type`**\n",
    "\n",
    "> The feature now consists of standardized one-length strings for encoding.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing String Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Single Strings to Binary Features**\n",
    "\n",
    "> The current values for \"neighbourhood_cleansed\" and 'host_verifications' are single string values consisting of unique values.\n",
    ">\n",
    ">**For each feature, I will:**\n",
    ">   * Separate each string into distinct, unique values;\n",
    ">   * Convert them into a binary column to represent whether or not that value is included in the listing;\n",
    ">   * Drop the old column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbourhood_Cleansed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Unique Neighborhoods**\n",
    "\n",
    "> Currently, the the `neighbourhood_cleansed` feature values consist of a single string of unique neighborhood names. This data is not currently usable for modeling and needs to be converted to be useful.\n",
    ">\n",
    "> **To convert this feature, I will:**\n",
    ">   * *Use a `.join()` method to create a single string of all of the values*\n",
    ">   * *Use a `.split()` method to split this string on each comma to divide them into separate strings*\n",
    ">   * *Convert the resulting list to a set to eliminate any duplicate names*\n",
    ">   * *Iterate over this set to remove any apostrophes or spaces in the names*\n",
    ">   * *Create new binary features for each neighborhood to indicate whether the processed name is in the given string value*\n",
    "\n",
    "At the end of this process, the dataframe will have a new binary column for each neighborhood.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.247480Z",
     "start_time": "2021-10-14T00:20:55.136Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting original feature values\n",
    "df.loc[:,'neighbourhood_cleansed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.249483Z",
     "start_time": "2021-10-14T00:20:55.145Z"
    }
   },
   "outputs": [],
   "source": [
    "## Create a set of unique neighborhood names\n",
    "unique_nghbrhd = set(','.join(df['neighbourhood_cleansed']).split(','))\n",
    "unique_nghbrhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.249483Z",
     "start_time": "2021-10-14T00:20:55.151Z"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning names and creating T/F binary columns\n",
    "\n",
    "for ngbrhd in unique_nghbrhd:\n",
    "    \n",
    "    ngbrhd = ngbrhd.replace(\"'\", \"\")\n",
    "    \n",
    "    if ngbrhd[0] == ' ':\n",
    "        ngbrhd = ngbrhd[1:]\n",
    "    \n",
    "    df[ngbrhd] = df['neighbourhood_cleansed'].str.contains(ngbrhd).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.250479Z",
     "start_time": "2021-10-14T00:20:55.156Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming new column names\n",
    "df.columns[42:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Post-Conversion**\n",
    "\n",
    "The resulting feature names and values are correct. **Now I will repeat the same process for the `host_verifications` feature.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host_Verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.253479Z",
     "start_time": "2021-10-14T00:20:55.166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting values\n",
    "df['host_verifications'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.254497Z",
     "start_time": "2021-10-14T00:20:55.171Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the first ten items of the second row\n",
    "\n",
    "df.loc[:,'host_verifications'][1][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.256484Z",
     "start_time": "2021-10-14T00:20:55.176Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting string value between verifications\n",
    "\n",
    "unique_verif = set(','.join(df['host_verifications']).split(','))\n",
    "unique_verif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_verif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Extra Cleaning**\n",
    "\n",
    "> These results are slightly different than the previous neighborhood values as they include brackets within the verifications as well as strings consisting of brackets only.\n",
    ">\n",
    "> **I will iterate over this set to remove the extra brackets, then create the new binary features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.257483Z",
     "start_time": "2021-10-14T00:20:55.181Z"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning names and creating T/F binary columns\n",
    "\n",
    "for verification in unique_verif:\n",
    "    \n",
    "    if len(verification) > 2:\n",
    "        \n",
    "        verification = verification.replace('[', '').replace(']', '').\\\n",
    "        replace(\"'\", '').replace('\"', '')\n",
    "\n",
    "    if verification[0] == ' ':\n",
    "        verification = verification[1:]\n",
    "\n",
    "        df[verification] = df['host_verifications'].str.\\\n",
    "                            contains(verification).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.258485Z",
     "start_time": "2021-10-14T00:20:55.186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting new column names\n",
    "df.columns[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping old features\n",
    "df = df.drop(columns = ['host_verifications', 'neighbourhood_cleansed'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: Binarizing Strings**\n",
    "\n",
    "> The string values for the `neighborhood_cleansed` and `host_verifications` features are now converted to new binary features to be used as part of my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 **Pre-Pipeline Review**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Final Review**\n",
    "\n",
    "> At this stage, I completed my data cleaning and preparation. I will review the data summaries once more before proceeding with my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewing string features\n",
    "eda.report_df(df.loc[:, df.columns[:40]].select_dtypes('O'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['host_response_time'].value_counts())\n",
    "display(df['room_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reviewing string features\n",
    "eda.report_df(df.loc[:, df.columns[:40]].select_dtypes('number'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Conclusion: Cleaning and Prep**\n",
    "\n",
    "> I am happy with the results of my cleaning process and I am ready to proceed with my modeling pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ Outlier Detection and Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Prior to my train/test split and further modeling, I will identify and remove listings with outlier values. These outliers may negatively affect my models' performance if retained.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing z-score outlier detection - arbitrarily-selected feature for testing purposes\n",
    "# df['price'][clf.find_outliers_z(df['price'])].index\n",
    "# len(df['price'][clf.find_outliers_z(df['price'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_counts = pd.DataFrame()\n",
    "\n",
    "# for col in df.select_dtypes('number').columns:\n",
    "#     outlier_counts[col] = clf.find_outliers_z(df[col]).sum()\n",
    "\n",
    "# outlier_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_counts = {}\n",
    "\n",
    "# for col in df.select_dtypes('number').columns:\n",
    "#     outlier_counts[col] = clf.find_outliers_z(df[col])\n",
    "\n",
    "# outlier_df = pd.DataFrame(outlier_counts)\n",
    "# outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_counts = {}\n",
    "\n",
    "# for col in df.select_dtypes('number').columns:\n",
    "#     outlier_counts[col] = clf.find_outliers_z(df[col])\n",
    "\n",
    "# outlier_df = pd.DataFrame(outlier_counts)\n",
    "# outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_df.any(axis=1)  ## t/f if a row has an outlier\n",
    "# outlier_df.mean() ## average % of outliers\n",
    "# outlier_df.mean()[outlier_df.mean() >= .05].sort_values(ascending=False) ## features with 5+% outlier values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_num_feats = df.select_dtypes('number').nunique()[df.select_dtypes('number').nunique() > 50].index.to_list()\n",
    "\n",
    "# outlier_counts = {}\n",
    "\n",
    "# for col in large_num_feats:\n",
    "#     outlier_counts[col] = clf.find_outliers_z(df[col])\n",
    "\n",
    "# outlier_df_big = pd.DataFrame(outlier_counts)\n",
    "# outlier_df_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier_df_big.any()\n",
    "# outlier_df_big.sum()\n",
    "# outlier_df_big.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Selecting features with more than X unique values\n",
    "# large_num_feats = df.select_dtypes('number').nunique()[df.select_dtypes('number').nunique() > 50].index.to_list()\n",
    "# len(large_num_feats)\n",
    "\n",
    "# ## Create list of indices with outlier values\n",
    "# outlier_feats = set()\n",
    "\n",
    "# for col in large_num_feats:\n",
    "#     outlier_feats.update(df[col][clf.find_outliers_z(df[col])].index.to_list())\n",
    "    \n",
    "# ## Percentage of observations with outliers\n",
    "# len(outlier_feats)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_no_outs = df.drop(outlier_feats).copy()\n",
    "# df_no_outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🪓 **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Before I run any further pre-processing, I split my data into training and test sets to allow me to test my model's performance.\n",
    ">\n",
    "> **Since my target feature is converted into binary values, I will use the \"stratify\" parameter in my train/test split, preserving the class balance when I split my data.** This will be key for proper evaluation of my models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.382526Z",
     "start_time": "2021-10-14T00:20:55.359482Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying features and target columns for dataset\n",
    "target = 'meets_score_threshold'\n",
    "\n",
    "X = df.drop(columns = target).copy()\n",
    "y = df[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.384478Z",
     "start_time": "2021-10-14T00:20:55.251Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming same number of rows\n",
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.385495Z",
     "start_time": "2021-10-14T00:20:55.257Z"
    }
   },
   "outputs": [],
   "source": [
    "## Performing split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚿 **Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">  Before I start my modeling processes, I convert my remaining categorical column via one-hot encoding and perform standardization on my numeric columns. Once my columns are properly converted, I will save them as new dataframes and use them in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.386481Z",
     "start_time": "2021-10-14T00:20:55.265Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes(include='number').columns.to_list()\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.388481Z",
     "start_time": "2021-10-14T00:20:55.271Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Specifying categorical columns for preprocessing\n",
    "cat_cols = X_train.select_dtypes(include='O').columns.to_list()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnning Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.390480Z",
     "start_time": "2021-10-14T00:20:55.281Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "\n",
    "### --- Creating column transformers --- ###\n",
    "\n",
    "## Imputing missing values\n",
    "missing_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "## Encoding categoricals - ignoring errors to prevent issues w/ test set\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore',\n",
    "                                        sparse=False)\n",
    "\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('imputer', missing_transformer),\n",
    "                            ('ohe', categorical_transformer)])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('imputer', missing_transformer),\n",
    "                            ('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('nums', num_pipe, num_cols),\n",
    "                  ('cats', cat_pipe, cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.391484Z",
     "start_time": "2021-10-14T00:20:55.285Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cats'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.392480Z",
     "start_time": "2021-10-14T00:20:55.290Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Transform the data via the ColumnTransformer preprocessor\n",
    "\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Resampling via SMOTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state = 42, n_jobs=-1)\n",
    "\n",
    "X_train_tf_df, y_train = smote.fit_sample(X_train_tf_df,y_train)\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.393480Z",
     "start_time": "2021-10-14T00:20:55.295Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "clf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test, \n",
    "                           metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.395481Z",
     "start_time": "2021-10-14T00:20:55.300Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving the baseline scores for later comparisons\n",
    "\n",
    "base_train_score, base_test_score, base_train_ll, base_test_ll = \\\n",
    "clf.model_scores(base, X_train_tf_df, y_train, X_test_tf_df, y_test)\n",
    "\n",
    "base_train_score, base_test_score, base_train_ll, base_test_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The baseline model is designed to be a poor-performer: the results are intended to be be close to .5 for most metrics, indicating the model is not performing better than simply guessing one result or the other.\n",
    ">\n",
    "> I use this model as a comparison point to judge the performance of my other models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  📊 **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "logreg = LogisticRegression(max_iter = 500, random_state = 42, n_jobs=-1)\n",
    "\n",
    "logreg.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evaluate_classification(logreg, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.plot_log_odds(logreg, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_params = {\n",
    "    'max_iter': [500, 750, 1000],\n",
    "    'C': [.01, 1, 10],\n",
    "    'solver': ['lbfgs','newton-cg']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LogReg Model\n",
    "lrgs = GridSearchCV(LogisticRegression(random_state = 42, n_jobs = -1), lg_params,\n",
    "                    scoring = 'average_precision', verbose = 2)\n",
    "\n",
    "lrgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evaluate_classification(lrgs, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg = lrgs.best_estimator_\n",
    "\n",
    "clf.plot_log_odds(best_logreg, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The simple LogReg model shows a slight performance increase - the log-loss decreased, the accuracy incrased, and my macro recall score also increased.\n",
    ">\n",
    "> This model mis-predicts values about 64% of the time, most likely due to the class imbalances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.401480Z",
     "start_time": "2021-10-14T00:20:55.333Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.402481Z",
     "start_time": "2021-10-14T00:20:55.338Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T00:20:55.404481Z",
     "start_time": "2021-10-14T00:20:55.343Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFC GSCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_params = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [40,50, 60],\n",
    "              'min_samples_split': [2,3]\n",
    "}\n",
    "\n",
    "rfgs = GridSearchCV(RandomForestClassifier(random_state = 42, n_jobs=-1),\n",
    "                    rfc_params,scoring = 'average_precision',verbose = 2,\n",
    "                   cv = 3)\n",
    "\n",
    "rfgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfgs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rfc = rfgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.evaluate_classification(best_rfc, X_train = X_train_tf_df,y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'average precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New Importances\n",
    "clf.plot_importances(best_rfc, X_test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpreting Results**\n",
    "\n",
    "> My resulting feature importances show that **the strongest predictor of scores 4.8+ would be whether or not a host is a SuperHost.** This makes sense, as one of the requirements for a host to be a SuperHost is to maintain a 4.8+ score, in addition to other requirements.\n",
    ">\n",
    "> Following SuperHost status are the number of listings for a host. **If a host has a large number of properties, they would most likely be an established businessperson and would be committed to hospitality, versus someone just renting out a spare room.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Interpreting Results with SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **One of the downsides of tree-based models is the difficulty when interpreting the impact of a specific feature.** Feature importances from tree-based models indicate how often a feature was used to make a decision, but they do not indicate if that feature was more or less likely to predict the target feature.\n",
    ">\n",
    "> To interpret these results, I will utilize a visualization package called **SHAP** to produce \"Shapely values\" for each feature. These values indicate each feature's marginal contribution to the model - answering the question, \"*How well does the model perform with this feature than without?*\"\n",
    ">\n",
    ">Using tools within the package, I will focus on the `summary_plot`, which visualizes each feature's Shapely value and the feature's  specific values from low-high (relative to each feature).\n",
    ">\n",
    "> More information about SHAP:\n",
    "* [SHAP Documentation](https://shap.readthedocs.io/en/latest/?badge=latest)\n",
    "* [SHAP Repository](https://github.com/slundberg/shap)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Initializing Javascript for SHAP models\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generating a sample of the overall data for review:\n",
    "X_shap = shap.sample(X_test_tf_df, nsamples=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing an explainer with the RandomForestClassifier model\n",
    "t_explainer = shap.TreeExplainer(rfgs.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TreeExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The SHAP package includes a few different \"Explainer\" objects to \"explain\" the results of different types of models. Since I used a RandomForestClassifier, I will use the \"TreeExplainer\" to calculate my SHAP values for plotting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating SHAP values for sample test data\n",
    "shap_values = t_explainer.shap_values(X_shap)\n",
    "len(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing column names for visualization labels\n",
    "X_shap.rename(columns = lambda x: x.title().replace('_', ' '), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Better plot\n",
    "shap.summary_plot(shap_values[1],X_shap,max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💡 **Final Recommendations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Based on the results of my models, I would recommend for Airbnb to prioritize promoting hosts to SuperHost status.**  SuperHost status is the strongest predictor for the desired high scores, and it is realistic for Airbnb to invest in their development and support. The second- and third-strongest predictors are much more difficult (and unrealistic) for Airbnb and hosts to improve.\n",
    ">\n",
    "> For further development, I would do the following:\n",
    ">* **Include details from text reviews:** while the traditional survey questions are respected and informative, text-based reviews take precedence. In my experience in hotel operations, I would often get much more information from the written reviews, including nuances and specifics that the yes/no or 1-5 ratings miss.\n",
    ">* **Include other regions:** My current dataset focused only on the Washington, D.C. area. Due to different regional factors (social/economic demographics; legal restrictions; etc.), other markets may show other features to be more important than my results. Additionally, I would like to explore international data to compare with the domestic data.\n",
    "\n",
    "--- "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8d0f1384b2220f7174c36601a5b2e7486df4bb8b8506072f32ea2a94af5243f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('test_intel_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "170px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
