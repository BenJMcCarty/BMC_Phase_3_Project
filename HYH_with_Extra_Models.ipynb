{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏡 **Helping the Hosts:** Determining Airbnb Host Ratings 🏨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Phase 3 Project: Classification**\n",
    ">\n",
    "> **Author:** Ben McCarty\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**In a post-COVID world, hospitality faces challenges as travel restrictions are imposed and lifted (and then re-imposed).** Travel and tourism came to a crashing halt in 2020 and still face challenges in returning to pre-2020 business levels.\n",
    "\n",
    "As restless travelers look to escape the confines of their homes, they expect the same high-quality services and experiences as pre-COVID. Competition within the hospitality industry is stronger than ever, putting more pressure on businesses to keep and grow their customer base.\n",
    "\n",
    "**The main performance metric for every company involved in hospitality is guest satisfaction.** If a guest isn't satisfied, they are not likely to return for another visit and may share their experience with others, pushing away potential business.\n",
    "\n",
    "Airbnb hosts face the same challenges as traditional hotels in these aggressive and challenging market conditions. In order to maximize their profitability and to distinguish themselves from traditional hotels, **Airbnb needs to know which aspects of a host property are the strongest predictors of whether a guest will give a satisfaction score of 4.8 or higher (out of 5).**\n",
    "\n",
    "With this question in mind, I obtained data about Airbnb host properties from the [Inside Airbnb project](http://insideairbnb.com/get-the-data.html#:~:text=Washington%2C%20D.C.%2C%20District%20of%20Columbia%2C%20United%20States) for the Washington, D.C. area. The dataset includes details about the hosts themselves; property details (bedrooms, bathrooms, property types); and reservation availability.\n",
    "\n",
    "**Once I have the data readied, I will use machine learning modeling techniques to determine my most important features for the region.** Then I will provide my final recommendations on what Airbnb should do to maximize the likelihood of their hosts obtaining a score of 4.8 or greater.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📂 **Imports and Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:28.644544Z",
     "start_time": "2021-10-05T22:54:28.620544Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact_manual\n",
    "import missingno\n",
    "\n",
    "## SKLearn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, \\\n",
    "                                AdaBoostClassifier,GradientBoostingClassifier \n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:28.659544Z",
     "start_time": "2021-10-05T22:54:28.647547Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:28.675543Z",
     "start_time": "2021-10-05T22:54:28.662547Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Personal functions\n",
    "import clf_functions.functions as cf\n",
    "\n",
    "## Tools to reload personal functions when called - prevents errors\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport clf_functions.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Show Visualizations Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:28.690545Z",
     "start_time": "2021-10-05T22:54:28.678551Z"
    }
   },
   "outputs": [],
   "source": [
    "## Setting to control whether or not to show visualizations\n",
    "show_visualizations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.233543Z",
     "start_time": "2021-10-05T22:54:28.693546Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading data and saving to a DataFrame\n",
    "\n",
    "source = 'data/listings.csv.gz'\n",
    "\n",
    "data = pd.read_csv(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.294568Z",
     "start_time": "2021-10-05T22:54:29.235546Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Inspecting imported dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.309585Z",
     "start_time": "2021-10-05T22:54:29.298555Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The initial read of the dataset shows there are 74 features and 8,033 entries. A quick glance at the `.head()` gives a sample of the entries, showing that some of the features are not relevant to my analysis.\n",
    ">\n",
    "> I need to get a better idea of the statistics for the dataset, especially any missing values and the datatypes for each column. I need to pre-process this data before I can perform any modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👨‍💻 **Interactive Investigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To increase accessibility to the data, **I include a widget to allow the user to sort through the data interactively.** I use [**Jupyter Widgets**](https://ipywidgets.readthedocs.io/en/latest/index.html) to create this interactive report.\n",
    ">\n",
    ">**To use:** select which column by which you would like to sort from the dropdown menu, then click the \"Run Interact\" button.\n",
    ">\n",
    ">***Note about 'Drop_Cols' and Cols:*** these keyword arguments are used to allow the user to drop specific columns.\n",
    ">\n",
    "> **Only click the \"Drop_Cols\" option when specifying \"Cols\"!** Otherwise it will cause an error.\n",
    ">\n",
    ">The 'Cols' dropdown menu does not affect the resulting report; the data is filtered from the report prior to displaying the results. \n",
    ">\n",
    ">I chose to include this option for flexibility and adaptability, but it does have the unintended consequence of creating another drop-down menu. Please ignore this menu, as it does not provide any additional functionality. For future work, I will disable the menu to prevent confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.591483Z",
     "start_time": "2021-10-05T22:54:29.312029Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Running report on unfiltered dataset\n",
    "\n",
    "interact_manual(cf.sort_report, Sort_by=list(cf.report_df(data).columns),\n",
    "                Source=source);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.607141Z",
     "start_time": "2021-10-05T22:54:29.594449Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing percentages of datatypes\n",
    "dt_pct = pd.DataFrame(data.dtypes.value_counts(1)\\\n",
    "                      .map(lambda x: f'{x*100:.0f}')).rename({0:'% Overall'},\n",
    "                                                             axis=1)\n",
    "dt_pct.style.set_caption('Data Types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Feature Review**\n",
    "\n",
    "> After reviewing my data, I see there are several features that contain irrelevant entries (URLs, source data, meta-data) or text values that are too complicated for simple processing (such as host and listing descriptions).\n",
    ">\n",
    "> ***I will drop these columns for the second report to review the remaining data for further processing.***\n",
    "\n",
    "**Data Type Breakdown**\n",
    "\n",
    "> There is nearly a 50/50 split between numeric/non-numeric features. ***I will need to determine how to pre-process these non-numeric values prior to modeling***. My options include:\n",
    "* Breaking down the values into distinct categories\n",
    "* Converting values to numeric data types as appropriate\n",
    "\n",
    "**Next Steps**\n",
    "\n",
    "> I determined that there are many features to drop from the dataset as well as a large number of non-numeric features to review and convert to distinct categories for encoding.\n",
    ">\n",
    "> ***I will start by dropping the irrelevant columns; then I will review the remaining features and update as appropriate.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.622142Z",
     "start_time": "2021-10-05T22:54:29.611030Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying columns to drop\n",
    "\n",
    "drop = ['id', 'name', 'description', 'neighborhood_overview', 'host_name',\n",
    "        'host_about', 'host_location', 'neighbourhood', 'property_type',\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'picture_url','host_url',\n",
    "        'host_thumbnail_url','host_picture_url','calendar_last_scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.968141Z",
     "start_time": "2021-10-05T22:54:29.625139Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Creating updated interactive report\n",
    "\n",
    "interact_manual(cf.sort_report, Drop_Cols = True, Cols = drop,\n",
    "                Sort_by=list(cf.report_df(data).columns), Source=source);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Interpretation:**\n",
    ">\n",
    "> The report shows that the dataset has a big problem with missing values:\n",
    ">\n",
    "> * **Empty:**\n",
    ">   * `neighbourhood_group_cleansed`\n",
    ">   * `bathrooms`\n",
    ">   * `calendar_updated`\n",
    ">\n",
    ">\n",
    "> * **Nearly empty:**\n",
    ">  * `license`\n",
    ">\n",
    ">\n",
    "> * **Missing 26-39% of data:**\n",
    ">  * `host_about`\n",
    ">  * `neighborhood_overview`\n",
    ">  * `neighbourhood`\n",
    ">  * `host_response_time`\n",
    ">  * `host_response_rate`\n",
    ">  * `review_scores_value`\n",
    ">  * `review_scores_checkin`\n",
    ">  * `review_scores_location`\n",
    ">  * `review_scores_accuracy`\n",
    ">  * `review_scores_communication`\n",
    ">  * `review_scores_cleanliness`\n",
    ">  * `host_acceptance_rate`\n",
    ">  * `reviews_per_month`\n",
    ">  * `first_review`\n",
    ">  * `review_scores_rating`\n",
    ">  * `last_review`\n",
    ">\n",
    ">---\n",
    ">\n",
    "> I will need to address these missing values before processing with the modeling. A few options include:\n",
    ">\n",
    "> * **Filling with the string \"missing\"** to indicate the value was missing.\n",
    ">    * *I would be able to treat \"missing\" as a distinct category and use it for modeling as well.*\n",
    ">\n",
    ">\n",
    "> * **Dropping the rows with missing values.**\n",
    ">    * *This may negatively impact the accuracy of my results by overfitting to the training data.*\n",
    ">\n",
    ">\n",
    "> Due to the large percentages of features and properties that are missing data, I feel it is best to drop those features and property entries that are missing values instead of attempting to fill in the gaps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To get a better idea of the missing values, I create a visual of the values via the 'Missingno' package. This visualization package includes several options for visualizing the missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:29.984138Z",
     "start_time": "2021-10-05T22:54:29.970142Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visually inspecting missing values\n",
    "if show_visualizations == True:\n",
    "    missingno.bar(data, labels=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on this visualization, I see that **there is a consistent trend in missing values for review scores:** if a row is missing one review score, it seems to be missing all of them.\n",
    ">\n",
    "> Additionally, **there are many missing values for the response time, response rate, and acceptance rate.** I want to use these columns in my classification, so I will need to replace those missing values.\n",
    ">\n",
    "> After reviewing these details, **I feel more comfortable with the option of dropping those rows with missing review values.** I will drop the values as part of my overall classification process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧼 **Data Cleaning and EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Fixing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> This dataset is missing a significant number of values for different columns. **In order to perform any modeling, I will address these missing values first.**\n",
    ">\n",
    "> As described previously, I will drop those features and rows with high percentages of missing values. Then, I will be able to fill in the missing values for the 'beds'/'bedrooms’ features by cross-referencing the columns. If one of the rows is missing a value for one feature, but has a value for the other, I will simply fill in the missing value with the value from the other column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.032138Z",
     "start_time": "2021-10-05T22:54:29.986140Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping features with high percentages (25%+) of missing values\n",
    "\n",
    "drop_na_cols = []\n",
    "for col in data.columns:\n",
    "    if ((data[col].isna().sum()) / len(data[col])) > .25 and col != 'review_scores_rating':\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.048138Z",
     "start_time": "2021-10-05T22:54:30.034142Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Appending previous list of columns to drop (metadata, etc.)\n",
    "\n",
    "for col in drop:\n",
    "    if col not in drop_na_cols:\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.112205Z",
     "start_time": "2021-10-05T22:54:30.050138Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating new dataframe that does not include the features to drop\n",
    "df = data.drop(columns= drop_na_cols).copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.317151Z",
     "start_time": "2021-10-05T22:54:30.114206Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Inspecting values prior to dropping\n",
    "cf.report_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛌 Filling Beds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After dropping columns and rows with large percentages of missing values, I proceed to address the missing values in the 'Beds' and \"Bedrooms\" columns.\n",
    ">\n",
    "> As the values are similar between the two, I will compare the rows against each other. For each row, if there is a missing value in one column that is present in the other, I will fill the missing value with the value present in the other column.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.365147Z",
     "start_time": "2021-10-05T22:54:30.320153Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'beds' with values for 'bedrooms'\n",
    "\n",
    "for idx in list(df['beds'][df['beds'].isna()].index):\n",
    "    if df['bedrooms'][idx] > 0:\n",
    "        df['beds'][idx] = df['bedrooms'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.458665Z",
     "start_time": "2021-10-05T22:54:30.369148Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'bedrooms' with values for 'beds'\n",
    "\n",
    "for idx in list(df['bedrooms'][df['bedrooms'].isna()].index):\n",
    "    if df['beds'][idx] > 0:\n",
    "        df['bedrooms'][idx] = df['beds'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.474616Z",
     "start_time": "2021-10-05T22:54:30.461703Z"
    }
   },
   "outputs": [],
   "source": [
    "## Resetting the index after dropping rows\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.634557Z",
     "start_time": "2021-10-05T22:54:30.476615Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Confirming reduction in missing values for 'beds' and 'bedrooms'\n",
    "\n",
    "rpt_clean  = cf.report_df(df)\n",
    "rpt_clean[rpt_clean['null_sum'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.761078Z",
     "start_time": "2021-10-05T22:54:30.636561Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Identifying index for remaining missing row for \"beds\"\n",
    "\n",
    "nan_bed = list(df['beds'][df['beds'].isna() > 0].index)\n",
    "\n",
    "## Inspecting row with missing value for \"bed\"\n",
    "df.iloc[nan_bed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After cleaning the 'bed'/'bedroom' columns, I see that I have one remaining missing pair of columns in one row. Later on, I will fill that value with an imputer as part of my modeling pipeline process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚮 Dropping Rows with 6+ Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.793011Z",
     "start_time": "2021-10-05T22:54:30.764015Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(df.isna().sum(axis=1) > 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.857014Z",
     "start_time": "2021-10-05T22:54:30.796015Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Removing rows with 6+ null values\n",
    "\n",
    "df = df[df.isna().sum(axis=1) < 6]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:30.889011Z",
     "start_time": "2021-10-05T22:54:30.860012Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reviewing remaining missing values\n",
    "\n",
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.062012Z",
     "start_time": "2021-10-05T22:54:30.892017Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.report_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚮 Dropping Rows Without Target Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.078010Z",
     "start_time": "2021-10-05T22:54:31.064023Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking for rows missing target values\n",
    "nan_index = df['review_scores_rating'].isna()\n",
    "nan_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.125010Z",
     "start_time": "2021-10-05T22:54:31.081013Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting rows to be dropped for missing the target feature\n",
    "df[nan_index][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.141011Z",
     "start_time": "2021-10-05T22:54:31.128013Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping rows from main dataframe\n",
    "df.drop(df[nan_index].index, inplace=True)\n",
    "\n",
    "## Resetting the index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.282023Z",
     "start_time": "2021-10-05T22:54:31.143014Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reviewing results\n",
    "cf.report_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔨 **Fixing Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Making Changes**\n",
    "\n",
    "> Now that I addressed most of my missing values, I process the remaining features and columns to allow for the modeling process.\n",
    "\n",
    "**Conversions and Creations**\n",
    ">\n",
    ">* Convert features with \"t\" / \"f\" values to 1/0, respectively\n",
    ">\n",
    ">\n",
    ">* Convert the `price` values from strings to floats\n",
    ">\n",
    ">\n",
    ">* Create a new feature, `Years_Hosting`, based on the  `host_since` feature\n",
    ">\n",
    ">\n",
    ">* Convert `bathrooms_text` into a new `num_bathrooms` numeric feature\n",
    ">\n",
    ">\n",
    ">* Convert the `room_type` column values into simpler string values\n",
    ">\n",
    ">\n",
    "> * Separate each neighborhood in `neighbourhood_cleansed` to a standalone binary feature\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Inspecting Review Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Converting the Target**\n",
    "\n",
    "> Currently, the the target variable `review_scores_rating` is a range of values from zero to five. ***I need to convert these values into binary values to represent whether the rating meets and/or exceeds the threshold of 4.8 (indicated by a 0/1 value, negative/positive respectively).*** I will create a new column to represent this binary classification and will drop the original feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.297744Z",
     "start_time": "2021-10-05T22:54:31.284023Z"
    }
   },
   "outputs": [],
   "source": [
    "## Using np.select to reassign target values based on conditional evaluations\n",
    "\n",
    "cond = [df['review_scores_rating'] >= 4.8,\n",
    "        df['review_scores_rating'] < 4.8]\n",
    "\n",
    "choice = [1,0]\n",
    "\n",
    "df['meets_threshold'] = np.select(cond, choice, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.313473Z",
     "start_time": "2021-10-05T22:54:31.300475Z"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping original target\n",
    "try:\n",
    "    df = df.drop(columns = 'review_scores_rating')\n",
    "except:\n",
    "    print('Feature was previously dropped.')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.329463Z",
     "start_time": "2021-10-05T22:54:31.316471Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming removal\n",
    "'review_scores_rating' not in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.361464Z",
     "start_time": "2021-10-05T22:54:31.332463Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reviewing results to confirm only 0/1 values and inspecting balance\n",
    "threshold_counts = pd.DataFrame(df['meets_threshold']\\\n",
    "                                .value_counts(dropna=False, sort=False))\n",
    "threshold_counts = pd.concat([threshold_counts,\n",
    "                              df['meets_threshold'].value_counts(dropna=0,\n",
    "                                                                 normalize=1,\n",
    "                                                                 sort=0)],\n",
    "                             axis=1)\n",
    "threshold_counts.columns = ['Count','Percent']\n",
    "threshold_counts['Percent'] = threshold_counts['Percent']\\\n",
    "                                      .map(lambda x: int(round(x, 2)*100))\n",
    "threshold_counts.style.set_caption('Target Breakdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.547974Z",
     "start_time": "2021-10-05T22:54:31.363467Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualizing the overall distribution of ratings\n",
    "\n",
    "ax = sns.barplot(data = df, x = threshold_counts.index, y = threshold_counts['Percent'])#, hue= df['meets_threshold'])\n",
    "\n",
    "ax.set(title = 'Breakdown of \"Meets Threshold\"',\n",
    "       xlabel = 'Rating', ylabel = 'Percentage of Reviews',\n",
    "       xticklabels = [\"Below\", \"Meets/Exceeds\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Rating Distributions**\n",
    "\n",
    "> After processing the missing values and formatting the data, the values are properly converted into 0/1 values with **62% of the reviews at or below the target threshold of 4.8.**\n",
    ">\n",
    "> This imbalance is very important for the later modeling process. ***I will need to address the imbalance to ensure the best model performance.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting True/False Columns to Binary Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.595986Z",
     "start_time": "2021-10-05T22:54:31.550974Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating list of true/false features to convert to 1/0, respectively\n",
    "\n",
    "t_f_xf = []\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() == 2 and df[col].dtype == 'O':\n",
    "        print(col,\":\",df[col].unique())\n",
    "        t_f_xf.append(col)\n",
    "        \n",
    "t_f_xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.627985Z",
     "start_time": "2021-10-05T22:54:31.598988Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_vc = pd.DataFrame()\n",
    "for col in t_f_xf:\n",
    "    tf_vc = pd.concat([tf_vc, df[col].value_counts(normalize = 1, dropna=0, sort=0)], axis=1)\n",
    "\n",
    "tf_vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.659985Z",
     "start_time": "2021-10-05T22:54:31.630987Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting t/f to 1/0, respectively\n",
    "\n",
    "df.loc[:,t_f_xf] = df.loc[:,t_f_xf].replace({ 't' : 1, 'f' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.705985Z",
     "start_time": "2021-10-05T22:54:31.662987Z"
    }
   },
   "outputs": [],
   "source": [
    "## Verifying results\n",
    "cf.report_df(df[t_f_xf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Price to Float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.721985Z",
     "start_time": "2021-10-05T22:54:31.708985Z"
    }
   },
   "outputs": [],
   "source": [
    "df['price'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.754003Z",
     "start_time": "2021-10-05T22:54:31.724987Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting each value into a float for processing\n",
    "\n",
    "df['price'] = df['price'].map(lambda price: price[1:].replace(',','')).astype('float')\n",
    "df['price'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.769990Z",
     "start_time": "2021-10-05T22:54:31.756987Z"
    }
   },
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating \"Years_Hosting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Since the 'host_since' feature is clearly a date, I will create a separate feature for how many years of activity for each host.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.801986Z",
     "start_time": "2021-10-05T22:54:31.772987Z"
    }
   },
   "outputs": [],
   "source": [
    "df['years_hosting'] = df[\"host_since\"].map(lambda x: 2021- int(x.split(\"-\")[0]))\n",
    "df['years_hosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.817984Z",
     "start_time": "2021-10-05T22:54:31.804987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['years_hosting'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.833986Z",
     "start_time": "2021-10-05T22:54:31.819987Z"
    }
   },
   "outputs": [],
   "source": [
    "df['years_hosting'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> I successfully created the new feature to represent how long each host is active (up to 2021). I will be curious to see the impact of the years of experience on the overall rating at the end of my modeling process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bathrooms_Text to Num_Bathrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> In the raw data, the original \"bathrooms\" feature was empty and was dropped as part of processing missing data.\n",
    ">\n",
    "> **My goal is to convert the \"bathrooms_text\" feature into a new \"num_bathrooms\" feature to indicate the number of bathrooms at a host property.**\n",
    ">\n",
    "> I assume the number of bathrooms would have an impact on the rating . More bathrooms could mean more space/comfort for the guest, but could also cause an increase in price.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.848988Z",
     "start_time": "2021-10-05T22:54:31.835986Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking current values\n",
    "df['bathrooms_text'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.895986Z",
     "start_time": "2021-10-05T22:54:31.851989Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the rows in which there are null values\n",
    "df[df['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.911986Z",
     "start_time": "2021-10-05T22:54:31.897988Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling null values with unique string ('Baths' not present otherwise)\n",
    "## Unique string can be used later to check for any other zero baths\n",
    "\n",
    "df['bathrooms_text'] = df['bathrooms_text'].fillna('0 Baths')\n",
    "df['bathrooms_text'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.942985Z",
     "start_time": "2021-10-05T22:54:31.913987Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting each list into separate strings\n",
    "df['num_bathrooms'] = df['bathrooms_text'].map(lambda x: x.split(' ')[0])\n",
    "df['num_bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **I will replace these values with the numeric value .5 as they are half-baths.** This will allow me to convert the column datatype to a float and use the column more easily in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:31.974511Z",
     "start_time": "2021-10-05T22:54:31.944987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Replacing string values with .5 to represent half-bathrooms\n",
    "\n",
    "replace = {'Half-bath': .5, 'Shared': .5}\n",
    "\n",
    "df['num_bathrooms'] = df['num_bathrooms'].replace(replace).astype(float)\n",
    "\n",
    "## Inspecting resulting values\n",
    "print(df['num_bathrooms'].dtype)\n",
    "df['num_bathrooms'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.022510Z",
     "start_time": "2021-10-05T22:54:31.977514Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting listings with more than 10 rooms\n",
    "df[df['num_bathrooms'] >10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After taking a look at the locations listed above on Google Maps (using their latitude/longitude), I feel like these three listings with more than 10 bathrooms are either duplicates or incorrect values (for 50 baths).\n",
    ">\n",
    "> Due to the questionable nature of these values, I will drop these rows to prevent these outliers from impacting my results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.070038Z",
     "start_time": "2021-10-05T22:54:32.029512Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting rows where 'num_bathrooms' is zero to validate data\n",
    "\n",
    "df[(df['num_bathrooms'] ==0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.086021Z",
     "start_time": "2021-10-05T22:54:32.073023Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing old column post-conversion\n",
    "df = df.drop(columns = 'bathrooms_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.102020Z",
     "start_time": "2021-10-05T22:54:32.088021Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming removal\n",
    "\n",
    "'bathrooms_text' in df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> My review of the original bathroom text for the zero bathrooms column shows that the listings are associated with a private room. This would make sense as the listings may not include an option such as a shared bath, etc..\n",
    ">\n",
    "> Additionally I did fill 9 instances of missing values with \"0 Baths,\" which would contribute slightly to this count.\n",
    ">\n",
    "> Overall, I feel the data is valid and I will use it for my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Room_Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">  In order to use “room_type” as a categorical variable, I convert the values to standardized strings. This allows me to perform one-hot encoding as part of my pre-modeling steps below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.118021Z",
     "start_time": "2021-10-05T22:54:32.104031Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reviewing pre-existing values\n",
    "\n",
    "df['room_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.134129Z",
     "start_time": "2021-10-05T22:54:32.120023Z"
    }
   },
   "outputs": [],
   "source": [
    "## Replacing values with updated strings\n",
    "\n",
    "replace_rooms = {'Entire home/apt': 'entire_home', \n",
    "                 'Private room': 'private_room',\n",
    "                 'Shared room': 'shared_room',\n",
    "                 'Hotel room': 'hotel_room'\n",
    "                }\n",
    "\n",
    "df['room_type'].replace(replace_rooms, inplace=True)\n",
    "df['room_type'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarizing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The current values for \"neighbourhood_cleansed\", 'host_verifications', and 'amenities' are  single string values. **For each feature, I will separate each string into distinct, unique values and convert them into a binary column to represent whether or not that value is included in the listing, then drop the old column.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbourhood_Cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.196061Z",
     "start_time": "2021-10-05T22:54:32.136130Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting feature\n",
    "df.loc[:,'neighbourhood_cleansed'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.212079Z",
     "start_time": "2021-10-05T22:54:32.198061Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:,'neighbourhood_cleansed'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.227596Z",
     "start_time": "2021-10-05T22:54:32.215077Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting string value between neighborhoods\n",
    "\n",
    "unique_neighborhood = list(set(','.join(df['neighbourhood_cleansed']).split(',')))\n",
    "unique_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.731257Z",
     "start_time": "2021-10-05T22:54:32.229574Z"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning names and creating T/F binary columns\n",
    "\n",
    "for neighborhood in unique_neighborhood:\n",
    "    \n",
    "    neighborhood = neighborhood.replace(\"'\", \"\")\n",
    "    \n",
    "    if neighborhood[0] == ' ':\n",
    "        neighborhood = neighborhood[1:]\n",
    "    \n",
    "    df[neighborhood] = df['neighbourhood_cleansed'].str\\\n",
    "                                           .contains(neighborhood).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.747263Z",
     "start_time": "2021-10-05T22:54:32.733199Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming results\n",
    "df.columns[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.763227Z",
     "start_time": "2021-10-05T22:54:32.749234Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Confirming removal of leading spaces and any quotes\n",
    "\n",
    "df.columns[-20:][0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host_Verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.779273Z",
     "start_time": "2021-10-05T22:54:32.765229Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting values\n",
    "df['host_verifications'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.795262Z",
     "start_time": "2021-10-05T22:54:32.781267Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the first five items of the second row\n",
    "\n",
    "df.loc[:,'host_verifications'][1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.811314Z",
     "start_time": "2021-10-05T22:54:32.797260Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Splitting string value between verifications\n",
    "\n",
    "unique_verif = list(set(','.join(df['host_verifications']).split(',')))\n",
    "unique_verif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.921348Z",
     "start_time": "2021-10-05T22:54:32.813245Z"
    }
   },
   "outputs": [],
   "source": [
    "## Cleaning names and creating T/F binary columns\n",
    "\n",
    "for verification in unique_verif:\n",
    "    \n",
    "    if len(verification) > 2:\n",
    "        \n",
    "        verification = verification.replace('[', '').replace(']', '').\\\n",
    "        replace(\"'\", '').replace('\"', '')\n",
    "\n",
    "    if verification[0] == ' ':\n",
    "        verification = verification[1:]\n",
    "\n",
    "        df[verification] = df['host_verifications'].str.\\\n",
    "                            contains(verification).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.937309Z",
     "start_time": "2021-10-05T22:54:32.924327Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> At this point, I successfully processed the 'host_verification' feature into distinct categories for modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amenities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.952767Z",
     "start_time": "2021-10-05T22:54:32.939278Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Inspecting values\n",
    "df['amenities'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:32.968499Z",
     "start_time": "2021-10-05T22:54:32.954773Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inspecting the first five items of the second row\n",
    "\n",
    "df.loc[:,'amenities'][1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:33.029270Z",
     "start_time": "2021-10-05T22:54:32.971254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Splitting string value between items\n",
    "\n",
    "unique_amenities = list(set(','.join(df['amenities']).split(',')))\n",
    "unique_amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:37.001023Z",
     "start_time": "2021-10-05T22:54:33.031164Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Cleaning names and creating T/F binary columns\n",
    "\n",
    "for amenity in unique_amenities:\n",
    "           \n",
    "    amenity = amenity.replace('[', '').replace(']', '').\\\n",
    "    replace(\"'\", '').replace('\"', '')\n",
    "\n",
    "    if amenity[0] == ' ':\n",
    "        amenity = amenity[1:]\n",
    "\n",
    "        df[amenity] = df['amenities'].str.\\\n",
    "                            contains(amenity).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:37.016233Z",
     "start_time": "2021-10-05T22:54:37.002933Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 **Pre-Pipeline Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:37.406564Z",
     "start_time": "2021-10-05T22:54:37.020204Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Review remaining data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:37.454469Z",
     "start_time": "2021-10-05T22:54:37.409567Z"
    }
   },
   "outputs": [],
   "source": [
    "## Removing columns with no impact on modeling\n",
    "\n",
    "df.drop(columns = ['host_since', 'host_neighbourhood', 'amenities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.362390Z",
     "start_time": "2021-10-05T22:54:37.456471Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Final review\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🪓 **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Before I run any further pre-processing, I split my data into training and test sets to allow me to test my model's performance.\n",
    ">\n",
    "> **Since my target feature is converted into binary values, I will use the \"stratify\" parameter in my train/test split, preserving the class balance when I split my data.** This will be key for proper evaluation of my models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.394397Z",
     "start_time": "2021-10-05T22:54:39.364390Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying features and target columns for dataset\n",
    "target = 'meets_threshold'\n",
    "\n",
    "X = df.drop(columns = target).copy()\n",
    "y = df[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.410389Z",
     "start_time": "2021-10-05T22:54:39.396390Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming same number of rows\n",
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.442345Z",
     "start_time": "2021-10-05T22:54:39.412391Z"
    }
   },
   "outputs": [],
   "source": [
    "## Splitting to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚿 **Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">  Before I start my modeling processes, I convert my remaining categorical column via one-hot encoding and perform standardization on my numeric columns. Once my columns are properly converted, I will save them as new dataframes and use them in my modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.457675Z",
     "start_time": "2021-10-05T22:54:39.444375Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Specifying numeric columns for preprocessing\n",
    "num_cols = X_train.select_dtypes(include=[int, float]).columns.to_list()\n",
    "# num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.473280Z",
     "start_time": "2021-10-05T22:54:39.459673Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Specifying categorical columns for preprocessing\n",
    "cat_cols = ['room_type']\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.520010Z",
     "start_time": "2021-10-05T22:54:39.475259Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking missing X-values for imputation\n",
    "X_train.isna().sum()[X_train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnning Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.582252Z",
     "start_time": "2021-10-05T22:54:39.522990Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "\n",
    "### --- Creating column transformers --- ###\n",
    "\n",
    "# Filling missing values in \"Beds\" and \"Bedrooms\"\n",
    "miss_num_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "## Encoding categoricals - ignoring errors to prevent issues w/ test set\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "\n",
    "### --- Creating column pipelines --- ###\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('ohe', categorical_transformer)])\n",
    "\n",
    "num_pipe = Pipeline(steps=[('imputer', miss_num_transformer),\n",
    "                           ('scaler', StandardScaler())])\n",
    "\n",
    "## Instantiating the ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('nums', num_pipe, num_cols),\n",
    "                  ('cats', cat_pipe, cat_cols)])\n",
    "\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:39.884243Z",
     "start_time": "2021-10-05T22:54:39.584241Z"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting feature preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "## Getting feature names from OHE\n",
    "ohe_cat_names = preprocessor.named_transformers_['cats'].named_steps['ohe'].get_feature_names(cat_cols)\n",
    "\n",
    "## Generating list for column index\n",
    "final_cols = [*num_cols, *ohe_cat_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:40.814128Z",
     "start_time": "2021-10-05T22:54:39.886244Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Transform the data via the ColumnTransformer preprocessor\n",
    "\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:41.615790Z",
     "start_time": "2021-10-05T22:54:40.817130Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_tf_df, y_train)\n",
    "\n",
    "cf.evaluate_classification(base,X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test, \n",
    "                           metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:41.646941Z",
     "start_time": "2021-10-05T22:54:41.617804Z"
    }
   },
   "outputs": [],
   "source": [
    "## Saving the baseline scores for later comparisons\n",
    "\n",
    "base_train_score, base_test_score, base_train_ll, base_test_ll = \\\n",
    "cf.model_scores(base, X_train_tf_df, y_train, X_test_tf_df, y_test)\n",
    "\n",
    "base_train_score, base_test_score, base_train_ll, base_test_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The baseline model is designed to be a poor-performer: the results are intended to be be close to .5 for most metrics, indicating the model is not performing better than simply guessing one result or the other.\n",
    ">\n",
    "> I use this model as a comparison point to judge the performance of my other models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  📊 **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:45.601441Z",
     "start_time": "2021-10-05T22:42:19.432582Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol = 1e-3, C = 10, penalty = \"l1\", solver = 'saga', \n",
    "                         max_iter=500, class_weight='balanced', n_jobs=-1,\n",
    "                         random_state = 42)\n",
    "\n",
    "clf.fit(X_train_tf_df, y_train)\n",
    "\n",
    "cf.evaluate_classification(clf, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "> The simple LogReg model shows a slight performance increase - the log-loss decreased, the accuracy incrased, and my macro recall score also increased.\n",
    ">\n",
    "> This model mis-predicts values about 64% of the time, most likely due to the class imbalances.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:53.221134Z",
     "start_time": "2021-10-05T22:54:53.210852Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(bootstrap = False,max_features= 'sqrt', class_weight = 'balanced',\n",
    "                            n_jobs=-1, max_depth = 15, min_samples_leaf = 3,\n",
    "                            min_samples_split = 4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:54.266357Z",
     "start_time": "2021-10-05T22:54:53.608103Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:54:55.713212Z",
     "start_time": "2021-10-05T22:54:54.269358Z"
    }
   },
   "outputs": [],
   "source": [
    "cf.evaluate_classification(rfc, X_train = X_train_tf_df, y_train = y_train,\n",
    "                           X_test = X_test_tf_df, y_test = y_test,\n",
    "                          metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    " **Comparing with Logistic Regression Model**\n",
    ">\n",
    "> The Random Forest classification model shows a higher degree of over-fitting; this is to be expected for tree-style models.\n",
    ">\n",
    "> This model shows slight performance increases as well. The log-loss decreased slightly as well, and the main two metrics of macro recall and accuracy both increased slightly.\n",
    ">\n",
    "> I will use this model as my best-performing model and will use its feature importances for my recommendations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Now that I have the feature importances from my model, I interpret the results via visualizing the most important features and the target feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:47.946637Z",
     "start_time": "2021-10-05T22:42:47.629841Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.plot_importances(rfc, X_test_tf_df, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Interpreting Results**\n",
    "\n",
    "> My resulting feature importances show that **the strongest predictor of scores 4.8+ would be whether or not a host is a SuperHost.** This makes sense, as one of the requirements for a host to be a SuperHost is to maintain a 4.8+ score, in addition to other requirements.\n",
    ">\n",
    "> Following SuperHost status are the number of listings for a host. **If a host has a large number of properties, they would most likely be an established businessperson and would be committed to hospitality, versus someone just renting out a spare room.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💡 **Final Recommendations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Based on the results of my models, I would recommend for Airbnb to prioritize promoting hosts to SuperHost status.**  SuperHost status is the strongest predictor for the desired high scores, and it is realistic for Airbnb to invest in their development and support. The second- and third-strongest predictors are much more difficult (and unrealistic) for Airbnb and hosts to improve.\n",
    ">\n",
    "> For further development, I would do the following:\n",
    ">* **Include details from text reviews:** while the traditional survey questions are respected and informative, text-based reviews take precedence. In my experience in hotel operations, I would often get much more information from the written reviews, including nuances and specifics that the yes/no or 1-5 ratings miss.\n",
    ">* **Include other regions:** My current dataset focused only on the Washington, D.C. area. Due to different regional factors (social/economic demographics; legal restrictions; etc.), other markets may show other features to be more important than my results. Additionally, I would like to explore international data to compare with the domestic data.\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing Models - Poorer Performances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The models below showed poorer performance versus my Logistic Regression and my Random Forest models. I include them for reference and example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:47.961637Z",
     "start_time": "2021-10-05T22:42:47.948639Z"
    }
   },
   "outputs": [],
   "source": [
    "# abc = AdaBoostClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:47.977637Z",
     "start_time": "2021-10-05T22:42:47.963638Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# abc.fit(X_train_tf_df, y_train)\n",
    "\n",
    "# cf.evaluate_classification(abc, X_train = X_train_tf_df, y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:47.993637Z",
     "start_time": "2021-10-05T22:42:47.980642Z"
    }
   },
   "outputs": [],
   "source": [
    "# gbc = GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.008637Z",
     "start_time": "2021-10-05T22:42:47.995638Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gbc.fit(X_train_tf_df, y_train)\n",
    "\n",
    "# cf.evaluate_classification(gbc, X_train = X_train_tf_df, y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.023637Z",
     "start_time": "2021-10-05T22:42:48.010637Z"
    }
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.039637Z",
     "start_time": "2021-10-05T22:42:48.025638Z"
    }
   },
   "outputs": [],
   "source": [
    "# lr_params = {\n",
    "#  'C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "#     'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
    "#     'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "#     'max_iter':[100, 200, 300, 400]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.055637Z",
     "start_time": "2021-10-05T22:42:48.041637Z"
    }
   },
   "outputs": [],
   "source": [
    "# gscv = GridSearchCV(LogisticRegression(class_weight='balanced'), lr_params, scoring = 'balanced_accuracy', cv=3,\n",
    "#                     n_jobs = -1)\n",
    "# gscv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.071635Z",
     "start_time": "2021-10-05T22:42:48.058640Z"
    }
   },
   "outputs": [],
   "source": [
    "# gscv.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.087637Z",
     "start_time": "2021-10-05T22:42:48.073638Z"
    }
   },
   "outputs": [],
   "source": [
    "# logreg_params = gscv.best_params_\n",
    "\n",
    "# logreg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.103644Z",
     "start_time": "2021-10-05T22:42:48.089638Z"
    }
   },
   "outputs": [],
   "source": [
    "# gscv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.119690Z",
     "start_time": "2021-10-05T22:42:48.105646Z"
    }
   },
   "outputs": [],
   "source": [
    "# cf.evaluate_classification(gscv.best_estimator_, X_train = X_train_tf_df, y_train = y_train,\n",
    "#                            X_test = X_test_tf_df, y_test = y_test,\n",
    "#                           metric = 'balanced accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSCV: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.135812Z",
     "start_time": "2021-10-05T22:42:48.122130Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_params = {\n",
    "#     'n_estimators':[100, 125, 150,],\n",
    "#     'max_depth': [10,20,30,40],\n",
    "#     'min_samples_split': [2,3,4],\n",
    "#     'min_samples_leaf': [1,2,3]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.151812Z",
     "start_time": "2021-10-05T22:42:48.137787Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier(class_weight = 'balanced',\n",
    "#                             n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.167195Z",
     "start_time": "2021-10-05T22:42:48.153780Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfgs = GridSearchCV(rfc, rfc_params, scoring = 'balanced_accuracy', cv=3,\n",
    "#                     n_jobs = -1)\n",
    "# rfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.183289Z",
     "start_time": "2021-10-05T22:42:48.169181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rfgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.198882Z",
     "start_time": "2021-10-05T22:42:48.186259Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_params = rfgs.best_params_\n",
    "\n",
    "# rfc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.214436Z",
     "start_time": "2021-10-05T22:42:48.201862Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.230618Z",
     "start_time": "2021-10-05T22:42:48.217448Z"
    }
   },
   "outputs": [],
   "source": [
    "# rfc_new = rfgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.245510Z",
     "start_time": "2021-10-05T22:42:48.233517Z"
    }
   },
   "outputs": [],
   "source": [
    "# cf.evaluate_classification(rfc_new, X_train_tf_df, y_train, X_test_tf_df, \n",
    "#                            y_test, 'recall (macro)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSCV: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.260583Z",
     "start_time": "2021-10-05T22:42:48.248514Z"
    }
   },
   "outputs": [],
   "source": [
    "# abc_params = {'n_estimators': [10,20, 30],\n",
    "# 'learning_rate': [0.0001, 0.001, 0.01, 0.1]}\n",
    "\n",
    "# # cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# abc = AdaBoostClassifier(DecisionTreeClassifier(),random_state=42)\n",
    "\n",
    "# abgs = GridSearchCV(estimator=abc, param_grid=abc_params, n_jobs=-1,\n",
    "#                            cv=3, scoring='balanced_accuracy', verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.276510Z",
     "start_time": "2021-10-05T22:42:48.268539Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# abgs.fit(X_train_tf_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.292513Z",
     "start_time": "2021-10-05T22:42:48.281513Z"
    }
   },
   "outputs": [],
   "source": [
    "# abc_best_params = abgs.best_params_\n",
    "\n",
    "# abc_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.308509Z",
     "start_time": "2021-10-05T22:42:48.295513Z"
    }
   },
   "outputs": [],
   "source": [
    "# abgs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.323510Z",
     "start_time": "2021-10-05T22:42:48.310510Z"
    }
   },
   "outputs": [],
   "source": [
    "# abc_new = abgs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.339511Z",
     "start_time": "2021-10-05T22:42:48.325513Z"
    }
   },
   "outputs": [],
   "source": [
    "# cf.evaluate_classification(abc_new, X_train_tf_df, y_train, X_test_tf_df, \n",
    "#                            y_test, 'recall (macro)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.354511Z",
     "start_time": "2021-10-05T22:42:48.342514Z"
    }
   },
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.369524Z",
     "start_time": "2021-10-05T22:42:48.356512Z"
    }
   },
   "outputs": [],
   "source": [
    "# xbgc = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.385510Z",
     "start_time": "2021-10-05T22:42:48.371512Z"
    }
   },
   "outputs": [],
   "source": [
    "# xbgc.fit(X = X_train_tf_df, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.400510Z",
     "start_time": "2021-10-05T22:42:48.387510Z"
    }
   },
   "outputs": [],
   "source": [
    "# xbgc.predict(X_test_tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.416510Z",
     "start_time": "2021-10-05T22:42:48.403513Z"
    }
   },
   "outputs": [],
   "source": [
    "# cf.evaluate_classification(xbgc,X_train_tf_df, y_train, X_test_tf_df, y_test, metric= 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.432512Z",
     "start_time": "2021-10-05T22:42:48.419514Z"
    }
   },
   "outputs": [],
   "source": [
    "# xgbc_names = xbgc.get_booster().feature_names\n",
    "# # xgbc_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.448511Z",
     "start_time": "2021-10-05T22:42:48.435514Z"
    }
   },
   "outputs": [],
   "source": [
    "# xgbc_importances = xbgc.feature_importances_\n",
    "# # xgbc_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.463513Z",
     "start_time": "2021-10-05T22:42:48.451513Z"
    }
   },
   "outputs": [],
   "source": [
    "# xgbc_results = pd.Series(data = xgbc_importances, index = xgbc_names)\n",
    "# xgbc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T22:42:48.478511Z",
     "start_time": "2021-10-05T22:42:48.466513Z"
    }
   },
   "outputs": [],
   "source": [
    "# xgbc_results.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_intel_env]",
   "language": "python",
   "name": "conda-env-test_intel_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.938px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
