{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔻 [Return to workflow](#leftoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏡 **AirBNB Dataset Review** 🏨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ Update target audience and guiding questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Who?**\n",
    ">* 🏢 **AirBNB Corporate** interested in maximizing customer satisfaction to increase repeat guests and encourage new guests to stay with AirBNB hosts\n",
    ">\n",
    ">\n",
    ">* **AirBNB hosts** interested in maximizing the ratings\n",
    "\n",
    "**Why?**\n",
    ">* 💰 **Revenue Management:** \n",
    ">\n",
    ">\n",
    ">\n",
    ">* 🤝 **Sales:**\n",
    ">\n",
    ">\n",
    ">\n",
    ">* 🛌 **Rooms Ops:**\n",
    ">\n",
    ">\n",
    ">\n",
    ">* 🍰 ☕ **Food and Beverage:**\n",
    ">\n",
    ">\n",
    ">\n",
    "\n",
    "**What?**\n",
    ">* 🧾 Dataset comprised of... \n",
    ">  * 32 different features\n",
    ">  * Nearly 120,000 reservation records\n",
    ">  * Source cited in Readme\n",
    "\n",
    "❌ **How?**\n",
    ">* Which models/methods? \n",
    ">* Data prep and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯  **Goal:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining whether or not a host location would receive a score greater than or equal to 4/5 (defined by `'review_scores_rating'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 **To-Do**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- [ ] [TD1](#td1)\n",
    "- [ ] [TD2](#td2)\n",
    "- [ ] [TD3](#td3)\n",
    "- [ ] [todo4](#td4)\n",
    "- [ ] [todo5](#td5)\n",
    "- [ ] [todo6](#td6)\n",
    "- [ ] [todo7](#td7)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📂 **Imports and Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:24.199493Z",
     "start_time": "2021-08-03T02:20:22.316506Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "## Visualizations\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact_manual\n",
    "import missingno\n",
    "\n",
    "## Modeling - SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB # for naive bayes model\n",
    "\n",
    "## Settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-talk')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "pd.set_option('max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:24.684896Z",
     "start_time": "2021-08-03T02:20:24.202345Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Personal functions\n",
    "import clf_functions.functions as cf\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport clf_functions.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Show Visualizations Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:24.700745Z",
     "start_time": "2021-08-03T02:20:24.687780Z"
    }
   },
   "outputs": [],
   "source": [
    "## Controlling whether or not to show visualizations\n",
    "show_visualizations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ FSDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:24.716664Z",
     "start_time": "2021-08-03T02:20:24.703666Z"
    }
   },
   "outputs": [],
   "source": [
    "# import fsds as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:24.732672Z",
     "start_time": "2021-08-03T02:20:24.719668Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fs.ihelp_menu([fs.ihelp_menu, sort_report])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📖 **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:27.861686Z",
     "start_time": "2021-08-03T02:20:24.735667Z"
    }
   },
   "outputs": [],
   "source": [
    "## Reading data and saving to a DataFrame\n",
    "\n",
    "source = 'http://data.insideairbnb.com/united-states/dc/washington-dc/2021-07-10/data/listings.csv.gz'\n",
    "\n",
    "data = pd.read_csv(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:27.924290Z",
     "start_time": "2021-08-03T02:20:27.863451Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Inspecting imported dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:27.940290Z",
     "start_time": "2021-08-03T02:20:27.927293Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The initial read of the dataset shows there are 74 features and 8,033 entries. A quick glance at the `.head()` gives a sample of the entries, showing that some of the features are not relevant to my analysis.\n",
    ">\n",
    "> I need to get a better idea of the statistics for the dataset, especially any missing values and the datatypes for each column. I need to pre-process this data before I can perform any modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 👨‍💻 **Interactive Investigation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To increase accessibility to the data, **I include a widget to allow the user to sort through the data interactively.** I use [**Jupyter Widgets**](https://ipywidgets.readthedocs.io/en/latest/index.html) to create this interactive report.\n",
    ">\n",
    ">**To use:** select which column by which you would like to sort from the dropdown menu, then click the \"Run Interact\" button.\n",
    ">\n",
    ">***Note about 'Drop_Cols' and Cols:*** these keyword arguments are used to allow the user to drop specific columns.\n",
    ">\n",
    "> **Only click the \"Drop_Cols\" option when specifying \"Cols\"!** Otherwise it will cause an error.\n",
    ">\n",
    ">The 'Cols' dropdown menu does not affect the resulting report; the data is filtered from the report prior to displaying the results. \n",
    ">\n",
    ">I chose to include this option for flexibility and adaptability, but it does have the unintended consequence of creating another drop-down menu. Please ignore this menu, as it does not provide any additional functionality. For future work, I will disable the menu to prevent confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.282279Z",
     "start_time": "2021-08-03T02:20:27.942291Z"
    }
   },
   "outputs": [],
   "source": [
    "## Running report on unfiltered dataset\n",
    "\n",
    "interact_manual(cf.sort_report, Sort_by=list(cf.report_df(data).columns),\n",
    "                Source=source);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.373278Z",
     "start_time": "2021-08-03T02:20:28.293565Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> After reviewing my data, I see there are several features that contain irrelevant entries (URLs, source data, meta data) or values that are too complicated for simple processing (such as host and listing descriptions).\n",
    ">\n",
    "> I will drop these columns for the second report to review the remaining data for further processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.389280Z",
     "start_time": "2021-08-03T02:20:28.376281Z"
    }
   },
   "outputs": [],
   "source": [
    "## Specifying columns to drop\n",
    "\n",
    "drop = ['id', 'host_id', 'name', 'description', 'neighborhood_overview', 'host_name',\n",
    "        'host_about', 'host_location', 'neighbourhood', 'property_type',\n",
    "        'listing_url', 'scrape_id', 'last_scraped', 'picture_url','host_url',\n",
    "        'host_thumbnail_url','host_picture_url','calendar_last_scraped']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.654221Z",
     "start_time": "2021-08-03T02:20:28.391279Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Creating updated interactive report\n",
    "\n",
    "interact_manual(cf.sort_report, Drop_Cols = True, Cols = drop,\n",
    "                Sort_by=list(cf.report_df(data).columns), Source=source);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Interpretation:**\n",
    ">\n",
    "> The report shows that the dataset has a big problem with missing values:\n",
    ">\n",
    "> * **Empty:**\n",
    ">   * `neighbourhood_group_cleansed`\n",
    ">   * `bathrooms`\n",
    ">   * `calendar_updated`\n",
    ">\n",
    ">\n",
    "> * **Nearly empty:**\n",
    ">  * `license`\n",
    ">\n",
    ">\n",
    "> * **Missing 26-39% of data:**\n",
    ">  * `host_about`\n",
    ">  * `neighborhood_overview`\n",
    ">  * `neighbourhood`\n",
    ">  * `host_response_time`\n",
    ">  * `host_response_rate`\n",
    ">  * `review_scores_value`\n",
    ">  * `review_scores_checkin`\n",
    ">  * `review_scores_location`\n",
    ">  * `review_scores_accuracy`\n",
    ">  * `review_scores_communication`\n",
    ">  * `review_scores_cleanliness`\n",
    ">  * `host_acceptance_rate`\n",
    ">  * `reviews_per_month`\n",
    ">  * `first_review`\n",
    ">  * `review_scores_rating`\n",
    ">  * `last_review`\n",
    ">\n",
    ">---\n",
    ">\n",
    "> I will need to address these missing values before processing with the modeling. A few options include:\n",
    ">\n",
    "> * **Filling with the string \"missing\"** to indicate the value was missing.\n",
    ">    * *I would be able to treat \"missing\" as a distinct category and use it for modeling as well.*\n",
    ">\n",
    ">\n",
    "> * **Dropping the rows with missing values.**\n",
    ">    * *This may negatively impact the accuracy of my results by overfitting to the training data.*\n",
    ">\n",
    ">\n",
    "> * I could **use the `SimpleImputer` tool from SKLearn to fill the missing values** with the mean, median, or mode values for each.\n",
    ">    * *I could couple this with a `GridSearchCV` to identify the method that has the strongest positive impact on my classification metrics.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> To get a better idea of the missing values, I create a visual of the values via the 'Missingno' package. This visualization package includes several options for visualizing the missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.669001Z",
     "start_time": "2021-08-03T02:20:28.656225Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Visually inspecting missing values\n",
    "if show_visualizations == True:\n",
    "    missingno.bar(data, labels=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Based on this visualization, I see that **there is a consistent trend in missing values for review scores:** if a row is missing one review score, it seems to be missing all of them.\n",
    ">\n",
    "> Additionally, **there are many missing values for the response time, response rate, and acceptance rate.** I want to use these columns in my classification, so I will need to replace those missing values.\n",
    ">\n",
    "> After reviewing these details, **I feel more comfortable with the option of dropping those rows with missing review values.** I will drop the values as part of my overall classification process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧼 **Data Cleaning and EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔎 Fixing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> This dataset is missing a significant number of values for different columns. **In order to perform any modeling, I will need to address these missing values first.**\n",
    ">\n",
    "> Depending on the feature and the number of missing values per row, I will take different approaches to keep as much data as possible and in its original state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.716235Z",
     "start_time": "2021-08-03T02:20:28.670984Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping features with high percentages (25%+) of missing values\n",
    "\n",
    "drop_na_cols = []\n",
    "for col in data.columns:\n",
    "    if ((data[col].isna().sum()) / len(data[col])) > .25 and col != 'review_scores_rating':\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.731231Z",
     "start_time": "2021-08-03T02:20:28.718207Z"
    }
   },
   "outputs": [],
   "source": [
    "## Appending previous list of columns to drop (metadata, etc.)\n",
    "\n",
    "for col in drop:\n",
    "    if col not in drop_na_cols:\n",
    "        drop_na_cols.append(col)\n",
    "\n",
    "drop_na_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.793795Z",
     "start_time": "2021-08-03T02:20:28.733203Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating new dataframe that does not include the features to drop\n",
    "df = data.drop(columns= drop_na_cols).copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:28.981177Z",
     "start_time": "2021-08-03T02:20:28.795821Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming dropped columns with high missing values\n",
    "cf.report_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.027175Z",
     "start_time": "2021-08-03T02:20:28.983182Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'beds' with values for 'bedrooms'\n",
    "\n",
    "for idx in list(df['beds'][df['beds'].isna()].index):\n",
    "    if df['bedrooms'][idx] > 0:\n",
    "        df['beds'][idx] = df['bedrooms'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.104176Z",
     "start_time": "2021-08-03T02:20:29.029178Z"
    }
   },
   "outputs": [],
   "source": [
    "## Filling missing values for 'bedrooms' with values for 'beds'\n",
    "\n",
    "for idx in list(df['bedrooms'][df['bedrooms'].isna()].index):\n",
    "    if df['beds'][idx] > 0:\n",
    "        df['bedrooms'][idx] = df['beds'][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.229088Z",
     "start_time": "2021-08-03T02:20:29.106177Z"
    }
   },
   "outputs": [],
   "source": [
    "## Confirming reduction in missing values for 'beds' and 'bedrooms'\n",
    "\n",
    "rpt_clean  = cf.report_df(df)\n",
    "rpt_clean[rpt_clean['null_sum'] >0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.260651Z",
     "start_time": "2021-08-03T02:20:29.233086Z"
    }
   },
   "outputs": [],
   "source": [
    "## Checking remaining missing values\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.306753Z",
     "start_time": "2021-08-03T02:20:29.263756Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Removing rows with 6+ null values\n",
    "\n",
    "df = df[df.isna().sum(axis=1) < 6]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.337689Z",
     "start_time": "2021-08-03T02:20:29.310758Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T19:24:46.416827Z",
     "start_time": "2021-08-02T19:24:46.402822Z"
    }
   },
   "source": [
    "---\n",
    "\n",
    "> At this point, **I cleaned up most of the null values via dropping columns with 25%+ missing values and dropping rows with 6+ missing values.**\n",
    ">\n",
    ">Additionally, **I filled missing values for 'beds'/'bedrooms' by checking the missing values for each column against the values in the other for each row.** If a row had a value in one of the columns but not the other, I filled the missing value with the value from the other column.\n",
    ">\n",
    "> Now, **I will take a different approach to my target feature, the \"review_scores_rating.\"** This feature contains a substantial number of missing values. Due to the impact of changing the values of my target variable, I will use a box plot to inspect the values, then determine whether to use the mean or median value to replace the missing values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:29.524167Z",
     "start_time": "2021-08-03T02:20:29.340644Z"
    }
   },
   "outputs": [],
   "source": [
    "## Visualizing the pre-processed target values\n",
    "\n",
    "sns.boxplot(data = df['review_scores_rating']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.085931Z",
     "start_time": "2021-08-03T02:20:29.527167Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df['review_scores_rating']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.413447Z",
     "start_time": "2021-08-03T02:20:30.087935Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df['review_scores_rating'][df['review_scores_rating'] >= 4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.428328Z",
     "start_time": "2021-08-03T02:20:30.415297Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculating mean for target values\n",
    "mean = df['review_scores_rating'].mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.443313Z",
     "start_time": "2021-08-03T02:20:30.430296Z"
    }
   },
   "outputs": [],
   "source": [
    "## Calculating median for target values\n",
    "median = df['review_scores_rating'].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    ">Based on this box plot and distributions, I see that **a large majority of the values are 4+, with a large number of values being 4.8+.**\n",
    ">\n",
    ">For the purposes of my classification (whether or not a given host would have a rating of 4+), I can fill the missing values with either the mean/median.\n",
    ">\n",
    "> **I will fill the missing values with the mean value of 4.68** to help ensure a fair representation of the overall data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.459298Z",
     "start_time": "2021-08-03T02:20:30.445296Z"
    }
   },
   "outputs": [],
   "source": [
    "df['review_scores_rating'].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.616625Z",
     "start_time": "2021-08-03T02:20:30.461298Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cf.report_df(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> At this point, I addressed most of the missing values in my dataset by dropping columns and filling missing values. There are still a few columns with missing values, but I will use a SimpleImputer combined with a GridSearchCV to determine the best method by which to fill those values.\n",
    ">\n",
    "> Now I will review the remaining data and determine if there are any other issues with my data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❌ 🔧 Changing DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.662247Z",
     "start_time": "2021-08-03T02:20:30.619623Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Reviewing the remaining dataframe\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENT:** what next? \n",
    "\n",
    "* DONE: T/F columns to 1/0\n",
    "\n",
    "\n",
    "* DONE: 'host_since' to DT\n",
    "\n",
    "\n",
    "* DONE: 'price' -$, to float\n",
    "\n",
    "\n",
    "* DONE: 'neighbourhood_cleansed' split on \", \" and convert to binary columns, then drop host_neighbourhood\n",
    "\n",
    "\n",
    "* 'bathrooms_text' split on space, keep 1st part, convert to int\n",
    "\n",
    "\n",
    "* 'host_verifications' - single string, needs extensive work in order to MLB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Converting True/False to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.677251Z",
     "start_time": "2021-08-03T02:20:30.665252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Creating list of true/false features to convert to 1/0, respectively\n",
    "\n",
    "t_f_xf = ['host_is_superhost','host_has_profile_pic','host_identity_verified',\n",
    "          'has_availability','instant_bookable']\n",
    "t_f_xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.708769Z",
     "start_time": "2021-08-03T02:20:30.679248Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Converting datatype to \"string\" to replace values\n",
    "\n",
    "df[t_f_xf] = df[t_f_xf].astype('str')\n",
    "df[t_f_xf].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.740770Z",
     "start_time": "2021-08-03T02:20:30.710773Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Converting t/f to 1/0, respectively\n",
    "\n",
    "df.loc[:,t_f_xf].replace({ 't' : 1, 'f' : 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.802769Z",
     "start_time": "2021-08-03T02:20:30.742770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Verifying results\n",
    "\n",
    "cf.report_df(df[t_f_xf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Price -$, to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.818770Z",
     "start_time": "2021-08-03T02:20:30.805769Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Converting each value into a float for processing\n",
    "\n",
    "try:\n",
    "    df['price'] = df['price'].map(lambda price: price[1:].replace(',','')).astype('float')\n",
    "    df['price'][0]\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary')\n",
    "    print(f\"\\nSample value: {df['price'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.834769Z",
     "start_time": "2021-08-03T02:20:30.820770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Host_Since to Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.866768Z",
     "start_time": "2021-08-03T02:20:30.836770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.896769Z",
     "start_time": "2021-08-03T02:20:30.868769Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,'host_since'] = pd.to_datetime(df.loc[:,'host_since'])\n",
    "df['host_since']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.912879Z",
     "start_time": "2021-08-03T02:20:30.899770Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[\"host_since\"].describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Host_Verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    ">Values are a single string and need broken up into usable pieces\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.928900Z",
     "start_time": "2021-08-03T02:20:30.914807Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test3 = df['host_verifications'][0]\n",
    "# test3[1:-1].replace('\"', \"'\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.944635Z",
     "start_time": "2021-08-03T02:20:30.930880Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # df['Tags'] = df.Tags.apply(lambda x: x[1:-1].split(','))\n",
    "\n",
    "# df['host_verifications'].apply(lambda x: x.split(','))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ TODO: Move MLB to preprocessor for neighbourhood_cleansed and Host_Verifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neighbourhood_cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.959666Z",
     "start_time": "2021-08-03T02:20:30.946636Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:,'neighbourhood_cleansed'][1].split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:30.991264Z",
     "start_time": "2021-08-03T02:20:30.962235Z"
    }
   },
   "outputs": [],
   "source": [
    "## Converting values into a list of strings for each neighborhood\n",
    "\n",
    "try:\n",
    "    df.loc[:,'neighbourhood_cleansed'] = df.loc[:,'neighbourhood_cleansed'] \\\n",
    "                                                .apply(lambda x: x.split(', '))\n",
    "    display(df.loc[:,'neighbourhood_cleansed'])\n",
    "except Exception:\n",
    "    print('\\nValues are already processed and saved. No changes necessary.')\n",
    "    print(f\"\\nSample value: {df.loc[:,'neighbourhood_cleansed'][3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The following code snippet is adapted from [here](https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list#:~:text=Sparse%20solution%20(for%20Pandas%20v0.25.0%2B)) by the user [Maxu](https://stackoverflow.com/users/5741205/maxu).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:31.021264Z",
     "start_time": "2021-08-03T02:20:30.993248Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "try:\n",
    "    df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('neighbourhood_cleansed')),\n",
    "                              columns=mlb.classes_,index=df.index))\n",
    "except Exception:\n",
    "        print('\\nValues are already processed and saved. No changes necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:31.113318Z",
     "start_time": "2021-08-03T02:20:31.023283Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bathrooms_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Goal: convert \"bathrooms_text\" into new \"num_bathrooms\" column to indicate number of bathrooms at a host property.\n",
    ">\n",
    "> The old \"bathrooms\" feature was empty and was dropped as part of processing missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:31.144309Z",
     "start_time": "2021-08-03T02:20:31.116230Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:34.783203Z",
     "start_time": "2021-08-03T02:20:34.774176Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[:,'bathrooms_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:20:35.497128Z",
     "start_time": "2021-08-03T02:20:35.351606Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df['bathrooms_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:21:45.559268Z",
     "start_time": "2021-08-03T02:21:45.541273Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[:,'bathrooms_text'].fillna('0 bath', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:21:46.177380Z",
     "start_time": "2021-08-03T02:21:46.148380Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.isna().sum()[df.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:21:54.478840Z",
     "start_time": "2021-08-03T02:21:54.467837Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,'bathrooms_text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:21:47.219874Z",
     "start_time": "2021-08-03T02:21:47.189873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,'bathrooms_text'] = df.loc[:,'bathrooms_text'].str.split(' ')\n",
    "df.loc[:,'bathrooms_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:22:21.602609Z",
     "start_time": "2021-08-03T02:22:21.333795Z"
    }
   },
   "outputs": [],
   "source": [
    "not_num = []\n",
    "\n",
    "for x in df.loc[:,'bathrooms_text'].index:\n",
    "    if len(df.loc[:,'bathrooms_text'][x][:1][0]) >3:\n",
    "           not_num += df.loc[:,'bathrooms_text'][x][:1]\n",
    "\n",
    "not_num = list(set(not_num))\n",
    "not_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:28:25.204068Z",
     "start_time": "2021-08-03T02:28:24.741899Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_bath = []\n",
    "\n",
    "for x in df.loc[:,'bathrooms_text'].index:\n",
    "    if len(df.loc[:,'bathrooms_text'][x][:1][0]) == 1:\n",
    "        if df.loc[:,'bathrooms_text'][x][0] == '0':\n",
    "            zero_bath.append(df.loc[:,'bathrooms_text'][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:28:51.259262Z",
     "start_time": "2021-08-03T02:28:51.237250Z"
    }
   },
   "outputs": [],
   "source": [
    "zero_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:10:17.900365Z",
     "start_time": "2021-08-03T02:10:17.625632Z"
    }
   },
   "outputs": [],
   "source": [
    "for x in df.loc[:,'bathrooms_text'].index:\n",
    "    if len(df.loc[:,'bathrooms_text'][x][:1][0]) >3:\n",
    "        df.loc[:,'bathrooms_text'][x] = [.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:10:18.198183Z",
     "start_time": "2021-08-03T02:10:17.902363Z"
    }
   },
   "outputs": [],
   "source": [
    "bathrooms = {}\n",
    "\n",
    "for x in df.loc[:,'bathrooms_text'].index:\n",
    "#     print(float(df.loc[:,'bathrooms_text'][x][:1][0]))\n",
    "#     df.assign({'bathrooms':float(df.loc[:,'bathrooms_text'][x][:1][0])})\n",
    "    bathrooms[x]=float(df.loc[:,'bathrooms_text'][x][:1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:10:18.213167Z",
     "start_time": "2021-08-03T02:10:18.200166Z"
    }
   },
   "outputs": [],
   "source": [
    "len(bathrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:10:46.187265Z",
     "start_time": "2021-08-03T02:10:46.164266Z"
    }
   },
   "outputs": [],
   "source": [
    "df['num_bathrooms'] = pd.Series(bathrooms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:11:04.907268Z",
     "start_time": "2021-08-03T02:11:04.888943Z"
    }
   },
   "outputs": [],
   "source": [
    "df['num_bathrooms'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:18:30.294053Z",
     "start_time": "2021-08-03T02:18:30.282052Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:18:31.854678Z",
     "start_time": "2021-08-03T02:18:31.844787Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zero_bath_idx = df.loc[:,'bathrooms_text'][df['num_bathrooms'] == 0].index.to_list()\n",
    "zero_bath_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T02:18:49.953393Z",
     "start_time": "2021-08-03T02:18:49.936392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.iloc[zero_bath_idx]['bathrooms_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Inspecting the Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> The target feature, `'review_scores_rating'`, is currently a range of values from 0 to 5, with 6% of the scores being 4 or above. The `.value_counts()` results show a value sub-zero; this is for the purpose of binning the values; the lowest value is actually 0.00.\n",
    ">\n",
    "> Of the scores less than 4, a little under half are between 3 and 4 (rounded) and about a third are between 0 and 1 (rounded).\n",
    ">\n",
    "> For my classification modeling, my classes are slightly imbalanced between values less/greater than 4 with a 32/68 split. I may need to perform some over-sampling of the minority class to increase my model's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🪓 **Train/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Before I run any further pre-processing, I split my data into training and test sets to allow me to test my model's performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.754946Z",
     "start_time": "2021-08-03T01:57:24.098Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Creating features/target for dataset\n",
    "# target = 'review_scores_rating'\n",
    "\n",
    "# X = df_cleaned.drop(columns = target).copy()\n",
    "# y = df_cleaned[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.756952Z",
     "start_time": "2021-08-03T01:57:24.101Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Confirming same number of rows\n",
    "# X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.757946Z",
     "start_time": "2021-08-03T01:57:24.106Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Splitting to prevent data leakage\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚿 **Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.759947Z",
     "start_time": "2021-08-03T01:57:24.109Z"
    }
   },
   "outputs": [],
   "source": [
    "# cat_cols = ['hotel', 'meal','arrival_date_month', 'country', 'market_segment',\n",
    "#             'distribution_channel','is_repeated_guest','reserved_room_type',\n",
    "#             'assigned_room_type','deposit_type', 'agent',\n",
    "#             'customer_type','reservation_status']\n",
    "\n",
    "# cont_cols = [col for col in X_train.drop(['reservation_status_date','company'],axis=1).columns if col not in cat_cols]\n",
    "\n",
    "# cont_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.760947Z",
     "start_time": "2021-08-03T01:57:24.112Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train[cat_cols] = X_train[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.762992Z",
     "start_time": "2021-08-03T01:57:24.115Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_test[cat_cols] = X_test[cat_cols].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.764986Z",
     "start_time": "2021-08-03T01:57:24.118Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Creating ColumnTransformer and sub-transformers for imputation and encoding\n",
    "\n",
    "# # Filling missing \"Children\"\n",
    "# zero_transformer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "\n",
    "# ##  \n",
    "# missing_transformer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "\n",
    "# ## Encoding categoricals - handling errors to prevent issues w/ test set\n",
    "# categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# cat_pipe = Pipeline(steps=[('imputer', missing_transformer),\n",
    "#                       ('ohe', categorical_transformer)])\n",
    "\n",
    "# cont_pipe = Pipeline(steps=[('imputer', zero_transformer),\n",
    "#                            ('scaler', StandardScaler())])\n",
    "\n",
    "# ## Instantiating the ColumnTransformer and including all transformers\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[('conts', cont_pipe, cont_cols),\n",
    "#                   ('cats', cat_pipe, cat_cols)])\n",
    "\n",
    "# preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.765985Z",
     "start_time": "2021-08-03T01:57:24.122Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessor.fit(X_train)\n",
    "\n",
    "# ## Getting feature names from OHE\n",
    "# ohe_cat_names = preprocessor.named_transformers_['cats'].named_steps['ohe'].get_feature_names(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-03T01:57:35.767985Z",
     "start_time": "2021-08-03T01:57:24.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Generating list for column index\n",
    "# final_cols = [*cont_cols, *ohe_cat_names]\n",
    "\n",
    "# ## Fit and transform the data via the ColumnTransformer\n",
    "# X_train_tf = preprocessor.transform(X_train)\n",
    "# X_train_tf_df = pd.DataFrame(X_train_tf, columns=final_cols, index=X_train.index)\n",
    "\n",
    "# ## Transforming the test set and saving\n",
    "# X_test_tf = preprocessor.transform(X_test)\n",
    "# X_test_tf_df = pd.DataFrame(X_test_tf, columns=final_cols, index=X_test.index)\n",
    "\n",
    "# display(X_train_tf_df.head(5),X_test_tf_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📝 Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process classification model - i.e. Logreg, KNN, DecisionTrees, etc.\n",
    "* Evaluate results\n",
    "* Determine if I need to redo pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚿 Classification Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env-bmc)",
   "language": "python",
   "name": "learn-env-bmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
